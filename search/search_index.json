{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Iris Agent Documentation","text":"<p>Welcome to the documentation for Iris Agent!</p> <p>Iris Agent is a lightweight, flexible, and provider-agnostic framework for building AI agents in Python. It simplifies the process of creating agents that can use tools, manage memory, and interact with various LLM providers.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Provider Agnostic: Works with OpenAI, Google Gemini, Anthropic, or any OpenAI-compatible API (like LocalAI or vLLM).</li> <li>Tool Support: Easy-to-use <code>@tool</code> decorator that automatically infers JSON schemas from Python type hints.</li> <li>Async &amp; Sync: Full <code>asyncio</code> support with a convenient synchronous wrapper.</li> <li>Streaming: Built-in support for streaming responses for real-time applications.</li> <li>Memory Management: Automatic conversation history management with support for custom memory stores.</li> <li>Prompt Management: Centralized <code>PromptRegistry</code> for reusable and template-based system prompts.</li> <li>Type Safety: Built with strict type hints for better developer experience and tooling support.</li> <li>Logging: Integrated Rich logging for beautiful, readable debug output.</li> </ul>"},{"location":"#architecture-overview","title":"Architecture Overview","text":"<p>Iris Agent is built around a few core components:</p> <pre><code>graph TD\n    A[Agent/AsyncAgent] --&gt; B[BaseLLMClient]\n    A --&gt; C[ToolRegistry]\n    A --&gt; D[PromptRegistry]\n    A --&gt; E[Memory List]\n    B --&gt; F[LLM Provider API]\n    C --&gt; G[Python Functions]</code></pre> <ul> <li>Agent: The central controller that orchestrates the LLM, tools, and memory.</li> <li>LLM Client: Handles communication with the AI provider.</li> <li>Tool Registry: Manages available tools and executes them when requested by the model.</li> <li>Prompt Registry: Stores system instructions and templates.</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<p>Here is a minimal example to get you running in seconds:</p> <pre><code>from iris_agent import Agent, BaseLLMClient, LLMConfig, LLMProvider\n\n# 1. Configure the LLM\nconfig = LLMConfig(\n    provider=LLMProvider.OPENAI,\n    model=\"gpt-4o\",\n    api_key=\"sk-...\"\n)\nclient = BaseLLMClient(config)\n\n# 2. Create the Agent\nagent = Agent(llm_client=client)\n\n# 3. Run\nresponse = agent.run(\"Hello! Who are you?\")\nprint(response)\n</code></pre>"},{"location":"#documentation-map","title":"Documentation Map","text":"<p>Explore the detailed documentation:</p> <ul> <li>Getting Started: Your first steps with Iris Agent.</li> <li>Installation: Setup guide for different environments.</li> <li>Core Concepts: Deep dive into Agents, Tools, and Memory.</li> <li>How-To Guides: Practical recipes for common tasks (Tools, Streaming, etc.).</li> <li>Modules Reference: Project structure overview.</li> <li>API Reference: Detailed class and function documentation.</li> <li>Examples: Code examples for various use cases.</li> <li>Troubleshooting: Solutions to common problems.</li> <li>FAQ: Frequently asked questions.</li> </ul>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the MIT License.</p>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/#agent","title":"Agent","text":""},{"location":"api/#iris_agentagent","title":"<code>iris_agent.Agent</code>","text":"<p>Synchronous wrapper around <code>AsyncAgent</code> for non-async usage.</p> <pre><code>class Agent:\n    def __init__(\n        self,\n        llm_client: BaseLLMClient,\n        prompt_registry: Optional[PromptRegistry] = None,\n        tool_registry: Optional[ToolRegistry] = None,\n        system_prompt_name: str = \"assistant\",\n        enable_logging: bool = False,\n        logger: Optional[logging.Logger] = None,\n    ) -&gt; None\n</code></pre> <p>Parameters:</p> <ul> <li><code>llm_client</code> (<code>BaseLLMClient</code>): The initialized LLM client instance.</li> <li><code>prompt_registry</code> (<code>PromptRegistry</code>, optional): Registry for system prompts. Defaults to a new empty registry.</li> <li><code>tool_registry</code> (<code>ToolRegistry</code>, optional): Registry for tools. Defaults to a new empty registry.</li> <li><code>system_prompt_name</code> (<code>str</code>, default=\"assistant\"): The key to look up in the prompt registry for the initial system message.</li> <li><code>enable_logging</code> (<code>bool</code>, default=False): If True, enables rich logging to stdout.</li> <li><code>logger</code> (<code>logging.Logger</code>, optional): Custom logger instance.</li> </ul>"},{"location":"api/#methods","title":"Methods","text":"<p><code>run(user_message: str | dict) -&gt; str</code></p> <p>Send a message to the agent and get a response.</p> <ul> <li><code>user_message</code>: The input text or a message dict (created via <code>create_message</code>).</li> <li>Returns: The assistant's response text.</li> </ul> <p><code>call_tool(name: str, **kwargs) -&gt; Any</code></p> <p>Manually call a tool registered with the agent.</p> <ul> <li><code>name</code>: Name of the tool.</li> <li><code>kwargs</code>: Arguments for the tool.</li> </ul>"},{"location":"api/#iris_agentasyncagent","title":"<code>iris_agent.AsyncAgent</code>","text":"<p>The core asynchronous agent class.</p> <pre><code>class AsyncAgent:\n    def __init__(\n        self,\n        llm_client: BaseLLMClient,\n        prompt_registry: Optional[PromptRegistry] = None,\n        tool_registry: Optional[ToolRegistry] = None,\n        system_prompt_name: str = \"assistant\",\n        enable_logging: bool = False,\n        logger: Optional[logging.Logger] = None,\n    ) -&gt; None\n</code></pre> <p>Same parameters as <code>Agent</code>.</p>"},{"location":"api/#methods_1","title":"Methods","text":"<p><code>async run(user_message: str | dict, ...) -&gt; str</code></p> <p>Run a single turn of conversation.</p> <ul> <li><code>user_message</code>: Input text or dict.</li> <li><code>json_response</code> (<code>bool</code>): If True, requests JSON output from the LLM.</li> <li><code>max_completion_tokens</code> (<code>int</code>): Cap the response length.</li> <li><code>seed</code> (<code>int</code>): Deterministic sampling seed.</li> <li><code>reasoning_effort</code> (<code>str</code>): Reasoning effort for models that support it (e.g. \"high\", \"medium\", \"low\").</li> <li><code>web_search_options</code> (<code>dict</code>): Search options for models that support it.</li> <li><code>extra_body</code> (<code>dict</code>): Provider-specific request body overrides.</li> </ul> <p><code>async run_stream(user_message: str | dict, ...) -&gt; AsyncGenerator[str, None]</code></p> <p>Stream the response token by token. Automatically handles tool calls in the background during the stream.</p> <p><code>run_stream(user_message: str | dict, ...) -&gt; Generator[str, None, None]</code></p> <p>Sync streaming helper on <code>Agent</code> that yields chunks by driving the async stream under the hood. This cannot be called from within an active event loop.</p>"},{"location":"api/#llm-client","title":"LLM Client","text":""},{"location":"api/#iris_agentbasellmclient","title":"<code>iris_agent.BaseLLMClient</code>","text":"<p>OpenAI-compatible client wrapper.</p> <pre><code>class BaseLLMClient:\n    def __init__(self, config: LLMConfig) -&gt; None\n</code></pre>"},{"location":"api/#iris_agentllmconfig","title":"<code>iris_agent.LLMConfig</code>","text":"<p>Configuration dataclass.</p> <pre><code>@dataclass\nclass LLMConfig:\n    provider: str\n    model: str\n    api_key: str | None = None\n    base_url: str | None = None\n    reasoning_effort: Optional[str] = None\n    web_search_options: Optional[dict] = None\n    extra_body: Optional[dict] = None\n</code></pre> <ul> <li><code>provider</code>: One of <code>LLMProvider</code> constants (e.g., <code>LLMProvider.OPENAI</code>).</li> <li><code>model</code>: Model identifier (e.g., \"gpt-4o\").</li> <li><code>base_url</code>: Optional override for compatible APIs.</li> <li><code>extra_body</code>: Optional provider-specific request body overrides.</li> </ul>"},{"location":"api/#tools","title":"Tools","text":""},{"location":"api/#iris_agenttool","title":"<code>iris_agent.tool</code>","text":"<p>Decorator to register a function as a tool.</p> <pre><code>@tool(name=\"custom_name\", description=\"Custom description\")\ndef my_function(arg: int): ...\n</code></pre>"},{"location":"api/#iris_agenttoolregistry","title":"<code>iris_agent.ToolRegistry</code>","text":"<p>Registry for managing tools.</p>"},{"location":"api/#methods_2","title":"Methods","text":"<p><code>register(func: Callable) -&gt; ToolSpec</code></p> <p>Register a single function.</p> <p><code>register_from(obj: Any) -&gt; None</code></p> <p>Scan an object (or module) for methods decorated with <code>@tool</code> and register them.</p>"},{"location":"api/#prompts","title":"Prompts","text":""},{"location":"api/#iris_agentpromptregistry","title":"<code>iris_agent.PromptRegistry</code>","text":"<p>Registry for managing system prompts.</p>"},{"location":"api/#methods_3","title":"Methods","text":"<p><code>add_prompt(name: str, template: str | Callable)</code></p> <p>Add a prompt.</p> <p><code>render(prompt_name: str, **kwargs) -&gt; str</code></p> <p>Render a prompt by name, passing kwargs to <code>format</code> or the callable.</p>"},{"location":"api/#messages","title":"Messages","text":""},{"location":"api/#iris_agentcreate_message","title":"<code>iris_agent.create_message</code>","text":"<p>Helper to create standard message dictionaries.</p> <pre><code>def create_message(\n    role: str,\n    content: str | None = None,\n    name: str | None = None,\n    images: List[str] | None = None\n) -&gt; dict\n</code></pre> <ul> <li><code>role</code>: \"user\", \"assistant\", \"system\", etc.</li> <li><code>content</code>: Text content.</li> <li><code>images</code>: List of image URLs.</li> </ul>"},{"location":"concepts/","title":"Core Concepts","text":"<p>Understanding how Iris Agent works under the hood will help you build more robust applications.</p>"},{"location":"concepts/#the-agent-loop","title":"The Agent Loop","text":"<p>At its core, an <code>Agent</code> (or <code>AsyncAgent</code>) implements a Loop that continues until the AI produces a final answer.</p> <ol> <li>Receive Input: The agent accepts a user message (text or multimodal).</li> <li>Update Memory: The message is added to the agent's conversation history (<code>self.memory</code>).</li> <li>LLM Call: The agent sends the full history + available tool definitions to the LLM.</li> <li>Decision: The LLM decides to either:<ul> <li>Respond: Generate a text response.</li> <li>Call Tool: Request execution of a specific tool with specific arguments.</li> </ul> </li> <li>Execution (if Tool):<ul> <li>The agent executes the requested Python function.</li> <li>The result is captured and added to memory as a <code>Tool</code> message.</li> <li>The loop repeats (Go to Step 3).</li> </ul> </li> <li>Final Output: When the LLM generates a text response (and no more tool calls), the text is returned to the user.</li> </ol>"},{"location":"concepts/#memory","title":"Memory","text":"<p>Iris Agent uses a simple list-based memory structure compatible with OpenAI's chat format.</p> <ul> <li>System Message: The initial instruction (e.g., \"You are a helpful assistant\"). Always kept at index 0.</li> <li>User Message: Input from the human.</li> <li>Assistant Message: Output from the AI.</li> <li>Tool Message: Results from function calls.</li> </ul> <pre><code>[\n  {\"role\": \"developer\", \"content\": \"You are a helpful assistant.\"},\n  {\"role\": \"user\", \"content\": \"What is the date?\"},\n  {\"role\": \"assistant\", \"tool_calls\": [...]},\n  {\"role\": \"tool\", \"tool_call_id\": \"...\", \"content\": \"2023-10-27\"},\n  {\"role\": \"assistant\", \"content\": \"It is October 27, 2023.\"}\n]\n</code></pre>"},{"location":"concepts/#tools-and-type-inference","title":"Tools and Type Inference","text":"<p>One of the most powerful features of Iris Agent is how it handles tools. Instead of manually writing JSON schemas, you write standard Python functions with type hints.</p>"},{"location":"concepts/#the-tool-decorator","title":"The <code>@tool</code> Decorator","text":"<p>When you decorate a function with <code>@tool</code>, Iris Agent inspects the signature:</p> <pre><code>@tool\ndef calculate_tax(amount: float, rate: float = 0.1) -&gt; float:\n    \"\"\"Calculate tax for a given amount.\"\"\"\n    return amount * rate\n</code></pre> <p>This is automatically converted to: <pre><code>{\n  \"name\": \"calculate_tax\",\n  \"description\": \"Calculate tax for a given amount.\",\n  \"parameters\": {\n    \"type\": \"object\",\n    \"properties\": {\n      \"amount\": {\"type\": \"number\"},\n      \"rate\": {\"type\": \"number\"}\n    },\n    \"required\": [\"amount\"]\n  }\n}\n</code></pre></p> <p>The <code>ToolRegistry</code> handles this conversion and the subsequent execution validation.</p>"},{"location":"concepts/#registries","title":"Registries","text":"<p>To keep your code organized, Iris Agent uses Registries.</p> <ul> <li><code>ToolRegistry</code>: A collection of available tools. You can share registries between multiple agents.</li> <li><code>PromptRegistry</code>: A collection of system prompts or templates. This allows you to dynamically switch personas or update prompts without changing application code.</li> </ul>"},{"location":"concepts/#llm-client-architecture","title":"LLM Client Architecture","text":"<p>The <code>BaseLLMClient</code> is the interface between your agent and the AI provider. It abstracts away provider-specific details and provides a unified API.</p>"},{"location":"concepts/#llmconfig","title":"LLMConfig","text":"<p>The <code>LLMConfig</code> dataclass contains all the information needed to connect to an LLM:</p> <ul> <li><code>provider</code>: The provider identifier (e.g., <code>LLMProvider.OPENAI</code>, <code>LLMProvider.GOOGLE</code>)</li> <li><code>model</code>: The specific model name (e.g., <code>\"gpt-4o\"</code>, <code>\"gpt-4o-mini\"</code>)</li> <li><code>api_key</code>: Your API key (can be <code>None</code> if using environment variables)</li> <li><code>base_url</code>: Optional override for custom endpoints (useful for local models, proxies, or Gemini via OpenAI-compatible API)</li> <li><code>reasoning_effort</code>: Optional hint for models that support reasoning (e.g., <code>\"high\"</code>, <code>\"medium\"</code>, <code>\"low\"</code>)</li> <li><code>web_search_options</code>: Optional configuration for models that support web search</li> <li><code>extra_body</code>: Optional provider-specific request overrides (e.g., Gemini <code>thinking_config</code>)</li> </ul>"},{"location":"concepts/#provider-compatibility","title":"Provider Compatibility","text":"<p>The <code>BaseLLMClient</code> uses the OpenAI SDK under the hood, which means it works with: - OpenAI: Direct API access - Google Gemini: Via OpenAI-compatible base URL (<code>https://generativelanguage.googleapis.com/v1beta/openai/</code>) - Local Models: Any OpenAI-compatible API (Ollama, vLLM, LocalAI, etc.)</p> <p>The client automatically handles differences in API capabilities (e.g., JSON mode support detection).</p>"},{"location":"concepts/#messages-and-roles","title":"Messages and Roles","text":"<p>Iris Agent uses a message-based conversation format compatible with OpenAI's chat completions API.</p>"},{"location":"concepts/#message-structure","title":"Message Structure","text":"<p>Messages are dictionaries with a <code>role</code> field and optional <code>content</code>, <code>name</code>, <code>images</code>, and <code>tool_calls</code> fields:</p> <pre><code>{\n  \"role\": \"user\",\n  \"content\": \"Hello!\",\n  \"name\": \"Alice\",  # Optional: for multi-user conversations\n  \"images\": [...]   # Optional: for multimodal messages\n}\n</code></pre>"},{"location":"concepts/#roles","title":"Roles","text":"<p>The framework supports five roles (defined in <code>Role</code> class):</p> <ul> <li><code>SYSTEM</code> / <code>DEVELOPER</code>: System instructions. The agent uses <code>DEVELOPER</code> by default for prompts from <code>PromptRegistry</code>. Both are treated identically and always kept at index 0 in memory.</li> <li><code>USER</code>: Human input messages.</li> <li><code>ASSISTANT</code>: AI-generated responses. Can include <code>tool_calls</code> when the model wants to use tools.</li> <li><code>TOOL</code>: Results from function calls. Must include <code>tool_call_id</code> to link back to the original request.</li> </ul>"},{"location":"concepts/#creating-messages","title":"Creating Messages","text":"<p>Use the <code>create_message()</code> helper function to create properly formatted messages:</p> <pre><code>from iris_agent import create_message, Role\n\n# Simple text message\nuser_msg = create_message(Role.USER, \"What's the weather?\")\n\n# Multimodal message with images\nimage_msg = create_message(\n    role=Role.USER,\n    content=\"Describe this image\",\n    images=[\"https://example.com/photo.jpg\"]\n)\n\n# Image-only message (no text)\nimage_only = create_message(\n    role=Role.USER,\n    content=\"\",  # Empty string for image-only\n    images=[\"https://example.com/chart.png\"]\n)\n</code></pre>"},{"location":"concepts/#tool-execution-and-error-handling","title":"Tool Execution and Error Handling","text":"<p>When the LLM requests a tool call, the agent:</p> <ol> <li>Parses Arguments: Extracts JSON arguments from the tool call</li> <li>Validates: Checks arguments against the tool's schema (type checking, required fields, enum values)</li> <li>Executes: Calls the Python function (sync or async)</li> <li>Handles Errors: If an exception occurs, the error message is captured and sent back to the LLM as the tool response, allowing the model to retry or adjust</li> </ol> <pre><code># If a tool raises an exception:\ntry:\n    result = await tool_registry.call_async(\"get_weather\", location=\"Tokyo\")\nexcept Exception as exc:\n    # The agent automatically wraps this as:\n    # {\"role\": \"tool\", \"content\": f\"Tool error: {exc}\"}\n</code></pre> <p>This error handling allows the LLM to understand what went wrong and potentially try a different approach.</p>"},{"location":"concepts/#streaming-architecture","title":"Streaming Architecture","text":"<p>When using <code>run_stream()</code>, the agent handles streaming differently from <code>run()</code>:</p> <ol> <li>Stream Collection: Instead of waiting for the full response, chunks are yielded immediately to the caller</li> <li>Tool Call Buffering: Tool calls arrive incrementally in the stream. The agent buffers these chunks until the stream completes</li> <li>Tool Execution: After the stream ends, if tool calls were detected, they are executed (just like in <code>run()</code>)</li> <li>Loop Continuation: The agent continues the loop, sending tool results back and streaming the next response</li> </ol> <p>This allows for real-time user feedback while still supporting the full tool-calling workflow.</p>"},{"location":"concepts/#advanced-llm-parameters","title":"Advanced LLM Parameters","text":"<p>The agent supports several advanced parameters that can be passed to <code>run()</code> or <code>run_stream()</code>:</p> <ul> <li><code>json_response</code>: Forces the model to output valid JSON only (requires model support)</li> <li><code>seed</code>: Sets a random seed for deterministic outputs (useful for testing)</li> <li><code>max_completion_tokens</code> / <code>max_tokens</code>: Limits the response length</li> <li><code>reasoning_effort</code>: Hints to models like o1 about how much reasoning to perform</li> <li><code>web_search_options</code>: Enables web search for models that support it</li> </ul> <p>These parameters are passed through to the underlying LLM client and may not be supported by all providers.</p>"},{"location":"concepts/#logging","title":"Logging","text":"<p>Iris Agent includes built-in support for Rich logging, which provides beautiful, colorized terminal output showing:</p> <ul> <li>User messages (cyan)</li> <li>LLM calls (dim)</li> <li>Tool calls (magenta)</li> <li>Tool responses (green)</li> <li>Assistant responses (bold green)</li> <li>Finish reasons (dim)</li> </ul> <p>Enable logging by setting <code>enable_logging=True</code> when creating an agent:</p> <pre><code>agent = Agent(..., enable_logging=True)\n</code></pre> <p>You can also provide a custom logger instance for integration with your existing logging infrastructure.</p>"},{"location":"concepts/#sync-vs-async","title":"Sync vs Async","text":"<ul> <li><code>AsyncAgent</code>: The \"real\" agent. It uses <code>asyncio</code> for non-blocking I/O (HTTP requests to LLMs, database calls in tools). Ideal for web servers (FastAPI, Quart). Supports both <code>run()</code> and <code>run_stream()</code>.</li> <li><code>Agent</code>: A synchronous wrapper around <code>AsyncAgent</code>. It manages the event loop for you. Ideal for scripts, CLI tools, or data science notebooks where <code>async/await</code> syntax might be cumbersome. Note: <code>Agent.run_stream()</code> is not supported; use <code>AsyncAgent</code> for streaming.</li> </ul>"},{"location":"examples/","title":"Examples","text":"<p>The <code>iris-agent</code> repository contains a rich set of examples in the <code>examples/</code> directory.</p>"},{"location":"examples/#featured-multi-agent-pattern","title":"Featured: Multi-Agent Pattern","text":"<p>Here is a complete example of a Planner/Executor pattern, where one agent creates a plan and another executes it.</p> <pre><code>import os\nfrom iris_agent import Agent, BaseLLMClient, LLMConfig, LLMProvider, PromptRegistry\n\ndef main():\n    api_key = os.getenv(\"OPENAI_API_KEY\")\n    if not api_key:\n        print(\"Set OPENAI_API_KEY first.\")\n        return\n\n    # 1. Setup Configuration\n    config = LLMConfig(\n        provider=LLMProvider.OPENAI,\n        model=\"gpt-4o\",\n        api_key=api_key\n    )\n    client = BaseLLMClient(config)\n    prompts = PromptRegistry()\n\n    # 2. Define Prompts\n    prompts.add_prompt(\"planner\", \"You are a strategic planner. Create a 3-step plan.\")\n    prompts.add_prompt(\"executor\", \"You are a doer. Execute the plan provided.\")\n\n    # 3. Initialize Agents\n    planner = Agent(llm_client=client, prompt_registry=prompts, system_prompt_name=\"planner\")\n    executor = Agent(llm_client=client, prompt_registry=prompts, system_prompt_name=\"executor\")\n\n    # 4. Run Workflow\n    task = \"Write a haiku about Python.\"\n\n    print(\"--- Planning ---\")\n    plan = planner.run(f\"Create a plan for: {task}\")\n    print(plan)\n\n    print(\"\\n--- Executing ---\")\n    final_result = executor.run(f\"Original Task: {task}\\n\\nPlan:\\n{plan}\")\n    print(final_result)\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"examples/#directory-structure","title":"Directory Structure","text":"<p>You can find more specific examples in the <code>examples/</code> folder of the repository:</p>"},{"location":"examples/#basic-usage-examples01_basic","title":"Basic Usage (<code>examples/01_basic/</code>)","text":"<ul> <li><code>simple_agent.py</code>: A minimal synchronous agent.</li> <li><code>async_agent.py</code>: The same agent using <code>asyncio</code>.</li> <li><code>streaming_agent.py</code>: How to stream tokens in real-time.</li> <li><code>sync_streaming_agent.py</code>: Sync streaming with <code>Agent.run_stream</code>.</li> </ul>"},{"location":"examples/#prompts-examples02_prompts","title":"Prompts (<code>examples/02_prompts/</code>)","text":"<ul> <li><code>prompt_registry_basic.py</code>: Registering and using static prompts.</li> <li><code>prompt_registry_dynamic.py</code>: Using templates (e.g., <code>{name}</code>) in system prompts.</li> </ul>"},{"location":"examples/#tools-examples03_tools","title":"Tools (<code>examples/03_tools/</code>)","text":"<ul> <li><code>tool_registry_basic.py</code>: Registering simple functions.</li> <li><code>async_tools.py</code>: Tools that perform async operations (e.g., <code>async def fetch_url(...)</code>).</li> <li><code>tool_schema_custom.py</code>: Advanced type hinting for tools.</li> </ul>"},{"location":"examples/#memory-examples04_memory","title":"Memory (<code>examples/04_memory/</code>)","text":"<ul> <li><code>memory_basics.py</code>: Inspecting and manipulating <code>agent.memory</code>.</li> </ul>"},{"location":"examples/#multi-agent-examples05_multi_agent","title":"Multi-Agent (<code>examples/05_multi_agent/</code>)","text":"<ul> <li><code>two_agents_chat.py</code>: Two agents talking to each other.</li> <li><code>planner_executor.py</code>: One agent creates a plan, another executes it (Orchestrator pattern).</li> </ul>"},{"location":"examples/#custom-llm-examples06_custom_llm","title":"Custom LLM (<code>examples/06_custom_llm/</code>)","text":"<ul> <li><code>mock_llm_client.py</code>: How to implement <code>BaseLLMClient</code> for testing or custom providers.</li> </ul>"},{"location":"examples/#logging-examples07_logging","title":"Logging (<code>examples/07_logging/</code>)","text":"<ul> <li><code>rich_logging.py</code>: Demonstration of the beautiful terminal output.</li> </ul>"},{"location":"examples/#messages-examples08_messages","title":"Messages (<code>examples/08_messages/</code>)","text":"<ul> <li><code>create_message_examples.py</code>: Creating user, assistant, and multimodal (image) messages.</li> </ul>"},{"location":"examples/#gemini-examples09_gemini","title":"Gemini (<code>examples/09_gemini/</code>)","text":"<ul> <li><code>gemini_basic.py</code>: Connecting to Google's Gemini models.</li> </ul>"},{"location":"faq/","title":"Frequently Asked Questions","text":""},{"location":"faq/#general","title":"General","text":""},{"location":"faq/#does-iris-agent-support-local-llms","title":"Does Iris Agent support local LLMs?","text":"<p>Yes. You can use any OpenAI-compatible API (Ollama, vLLM, LocalAI) by setting the <code>base_url</code> in <code>LLMConfig</code>.</p>"},{"location":"faq/#is-conversation-history-persisted-to-disk","title":"Is conversation history persisted to disk?","text":"<p>No. By default, memory is stored in a Python list (<code>agent.memory</code>). If you restart the script, memory is lost. You can easily implement persistence by saving/loading <code>agent.memory</code> to a JSON file or database.</p>"},{"location":"faq/#can-i-use-this-with-anthropic-claude","title":"Can I use this with Anthropic Claude?","text":"<p>Yes, but you currently need an adapter or a proxy that provides an OpenAI-compatible endpoint, OR you can implement a custom <code>BaseLLMClient</code> that calls the Anthropic SDK.</p>"},{"location":"faq/#technical","title":"Technical","text":""},{"location":"faq/#how-do-i-clear-the-agents-memory","title":"How do I clear the agent's memory?","text":"<p>Simply assign a new list or call clear on the existing one, but remember to keep the system prompt if you want it to persist.</p> <pre><code># Keep system prompt (index 0)\nagent.memory = [agent.memory[0]]\n</code></pre>"},{"location":"faq/#can-i-share-tools-between-agents","title":"Can I share tools between agents?","text":"<p>Yes. You can pass the same <code>ToolRegistry</code> instance to multiple agents.</p>"},{"location":"faq/#is-it-thread-safe","title":"Is it thread-safe?","text":"<p>The <code>AsyncAgent</code> is designed for <code>asyncio</code> concurrency. The synchronous <code>Agent</code> is not thread-safe if shared across threads without locks, as it modifies internal state (memory). It is better to create one agent per session/user.</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>This guide will help you build your first AI agent using Iris Agent.</p>"},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10 or higher.</li> <li>An API key for an LLM provider (e.g., OpenAI, Google Gemini, DeepSeek, etc.).</li> </ul>"},{"location":"getting-started/#1-installation","title":"1. Installation","text":"<p>First, install the package:</p> <pre><code>pip install iris-agent\n</code></pre>"},{"location":"getting-started/#2-basic-configuration","title":"2. Basic Configuration","text":"<p>To create an agent, you need an <code>LLMConfig</code> and a <code>BaseLLMClient</code>.</p> <pre><code>import os\nfrom iris_agent import Agent, BaseLLMClient, LLMConfig, LLMProvider\n\n# Ideally, load API keys from environment variables\napi_key = os.getenv(\"OPENAI_API_KEY\")\n\nconfig = LLMConfig(\n    provider=LLMProvider.OPENAI,\n    model=\"gpt-4o-mini\",\n    api_key=api_key\n)\n\nclient = BaseLLMClient(config)\n</code></pre>"},{"location":"getting-started/#3-creating-your-first-agent","title":"3. Creating Your First Agent","text":"<p>Combine the client with an <code>Agent</code> instance. You can also define a system prompt using the <code>PromptRegistry</code>.</p> <pre><code>from iris_agent import Agent, PromptRegistry\n\n# Optional: Define system instructions\nregistry = PromptRegistry()\nregistry.add_prompt(\"assistant\", \"You are a friendly pirate assistant. Arr!\")\n\nagent = Agent(\n    llm_client=client,\n    prompt_registry=registry,\n    system_prompt_name=\"assistant\"\n)\n\nresponse = agent.run(\"What is the capital of France?\")\nprint(response)\n# Output: Arr! The capital of France be Paris, matey!\n</code></pre>"},{"location":"getting-started/#4-adding-tools","title":"4. Adding Tools","text":"<p>Tools allow your agent to interact with the outside world. Use the <code>@tool</code> decorator.</p> <pre><code>from iris_agent import tool, ToolRegistry\n\ntool_registry = ToolRegistry()\n\n@tool\ndef get_weather(location: str) -&gt; str:\n    \"\"\"Get the current weather for a location.\"\"\"\n    # In a real app, call an API here\n    return f\"The weather in {location} is sunny and 25\u00b0C.\"\n\ntool_registry.register(get_weather)\n\n# Update the agent with the tool registry\nagent = Agent(\n    llm_client=client,\n    tool_registry=tool_registry\n)\n\nresponse = agent.run(\"What's the weather in Tokyo?\")\nprint(response)\n# The weather in Tokyo is sunny and 25\u00b0C.\n</code></pre>"},{"location":"getting-started/#5-async-support","title":"5. Async Support","text":"<p>For high-performance applications (e.g., web servers), use <code>AsyncAgent</code>.</p> <pre><code>import asyncio\nfrom iris_agent import AsyncAgent\n\nasync def main():\n    agent = AsyncAgent(llm_client=client)\n    response = await agent.run(\"Tell me a quick joke.\")\n    print(response)\n    # Hey, what do you call a function that only tells the truth? A literal... function. Whatever.\n\n\n\nasyncio.run(main())\n</code></pre>"},{"location":"getting-started/#6-sync-streaming","title":"6. Sync Streaming","text":"<p>If you prefer a synchronous interface but want streaming tokens, use <code>Agent.run_stream()</code>:</p> <pre><code>from iris_agent import Agent\n\nagent = Agent(llm_client=client)\nfor chunk in agent.run_stream(\"Tell me a short story.\"):\n    print(chunk, end=\"\", flush=True)\nprint()\n</code></pre>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Core Concepts like Memory and Tools.</li> <li>See more Examples including multi-agent setups.</li> <li>Check the API Reference for detailed documentation.</li> </ul>"},{"location":"how-to/","title":"How-To Guides","text":"<p>This section provides practical guides for common tasks.</p>"},{"location":"how-to/#how-to-define-tools","title":"How to Define Tools","text":"<p>Tools are the primary way your agent interacts with the world.</p>"},{"location":"how-to/#1-simple-function-tool","title":"1. Simple Function Tool","text":"<pre><code>from iris_agent import tool\n\n@tool\ndef add(a: int, b: int) -&gt; int:\n    \"\"\"Add two numbers.\"\"\"\n    return a + b\n</code></pre>"},{"location":"how-to/#2-tool-with-complex-types","title":"2. Tool with Complex Types","text":"<p>Iris Agent supports <code>List</code>, <code>Dict</code>, and <code>Literal</code> for validation.</p> <pre><code>from typing import Literal, List\nfrom iris_agent import tool\n\n@tool\ndef search_products(\n    query: str,\n    category: Literal[\"electronics\", \"clothing\", \"books\"],\n    tags: List[str] = None\n) -&gt; str:\n    \"\"\"Search for products in a specific category.\"\"\"\n    # Implementation...\n    return \"Found 5 items\"\n</code></pre>"},{"location":"how-to/#3-registering-tools","title":"3. Registering Tools","text":"<pre><code>from iris_agent import ToolRegistry\n\nregistry = ToolRegistry()\nregistry.register(add)\nregistry.register(search_products)\n\n# Pass registry to Agent\nagent = Agent(..., tool_registry=registry)\n</code></pre>"},{"location":"how-to/#how-to-stream-responses","title":"How to Stream Responses","text":"<p>Streaming is essential for a responsive UI. You can stream with <code>AsyncAgent</code> or use <code>Agent.run_stream()</code> for a sync-friendly iterator.</p> <pre><code>import asyncio\nfrom iris_agent import AsyncAgent\n\nasync def stream_chat():\n    agent = AsyncAgent(...)\n\n    # run_stream yields chunks of text\n    async for chunk in agent.run_stream(\"Tell me a story\"):\n        print(chunk, end=\"\", flush=True)\n\nasyncio.run(stream_chat())\n</code></pre> <p>Sync usage:</p> <pre><code>from iris_agent import Agent\n\nagent = Agent(...)\n\nfor chunk in agent.run_stream(\"Tell me a story\"):\n    print(chunk, end=\"\", flush=True)\n</code></pre>"},{"location":"how-to/#how-to-use-json-mode","title":"How to Use JSON Mode","text":"<p>If you need the agent to output strict JSON, use the <code>json_response</code> parameter.</p> <pre><code>async def get_json_data():\n    agent = AsyncAgent(...)\n\n    prompt = \"Generate a list of 3 cities with lat/long.\"\n\n    # Ensure you mention JSON in the prompt as well for best results\n    response = await agent.run(\n        prompt + \" Output JSON.\",\n        json_response=True\n    )\n    print(response)\n    # Output: {\"cities\": [...]}\n</code></pre>"},{"location":"how-to/#how-to-handle-images-multimodal","title":"How to Handle Images (Multimodal)","text":"<p>Use <code>create_message</code> to send images to models like GPT-4o or Gemini 1.5 Pro.</p> <pre><code>from iris_agent import create_message, Role\n\nmsg = create_message(\n    role=Role.USER,\n    content=\"What is in this image?\",\n    images=[\"https://example.com/photo.jpg\"]\n)\n\nresponse = agent.run(msg)\n</code></pre>"},{"location":"how-to/#how-to-use-gemini-openai-compatibility","title":"How to Use Gemini OpenAI Compatibility","text":"<p>Gemini models are accessible through the OpenAI-compatible endpoint. When you set <code>provider=LLMProvider.GOOGLE</code>, Iris Agent defaults to the Gemini OpenAI base URL and reads <code>GEMINI_API_KEY</code> if <code>api_key</code> is not provided.</p> <pre><code>import os\nfrom iris_agent import Agent, BaseLLMClient, LLMConfig, LLMProvider\n\nconfig = LLMConfig(\n    provider=LLMProvider.GOOGLE,\n    model=\"gemini-3-flash-preview\",\n    api_key=os.getenv(\"GEMINI_API_KEY\"),\n)\nclient = BaseLLMClient(config)\nagent = Agent(llm_client=client)\n\nresponse = agent.run(\"Explain how AI works.\")\nprint(response)\n</code></pre> <p>To send Gemini-specific fields (like <code>thinking_config</code>), pass <code>extra_body</code>:</p> <pre><code>response = agent.run(\n    \"Explain how AI works.\",\n    extra_body={\n        \"google\": {\n            \"thinking_config\": {\n                \"thinking_budget\": \"low\",\n                \"include_thoughts\": True,\n            }\n        }\n    },\n)\n</code></pre>"},{"location":"how-to/#how-to-use-prompt-registry","title":"How to Use Prompt Registry","text":"<p>The <code>PromptRegistry</code> is a powerful system for managing system instructions, personas, and dynamic prompt generation. It supports three types of prompts: static strings, template strings with placeholders, and callable functions.</p>"},{"location":"how-to/#why-use-promptregistry","title":"Why Use PromptRegistry?","text":"<p>Instead of hardcoding system prompts in your agent initialization, the <code>PromptRegistry</code> provides:</p> <ul> <li>Centralized Management: All prompts in one place, easy to update and version.</li> <li>Dynamic Generation: Create prompts on-the-fly based on context, user data, or configuration.</li> <li>Reusability: Share the same registry across multiple agents with different personas.</li> <li>Template Support: Use Python's <code>.format()</code> syntax for simple variable substitution.</li> </ul>"},{"location":"how-to/#1-static-prompts","title":"1. Static Prompts","text":"<p>The simplest form is a static string prompt:</p> <pre><code>from iris_agent import PromptRegistry, Agent\n\nregistry = PromptRegistry()\nregistry.add_prompt(\"assistant\", \"You are a helpful AI assistant.\")\n\n# Use it with an agent\nagent = Agent(..., prompt_registry=registry, system_prompt_name=\"assistant\")\n</code></pre>"},{"location":"how-to/#2-template-prompts-string-formatting","title":"2. Template Prompts (String Formatting)","text":"<p>You can use Python's <code>.format()</code> syntax to create dynamic prompts:</p> <pre><code>registry = PromptRegistry()\nregistry.add_prompt(\"personal_assistant\", \"You are {name}'s personal assistant. Today is {date}.\")\n\n# Render with variables\nprompt_text = registry.render(\"personal_assistant\", name=\"Alice\", date=\"2024-01-15\")\n# Result: \"You are Alice's personal assistant. Today is 2024-01-15.\"\n\n# Use with agent\nagent = Agent(..., prompt_registry=registry, system_prompt_name=\"personal_assistant\")\n# Note: For templates with variables, you may need to pre-render or use callable prompts\n</code></pre>"},{"location":"how-to/#3-callable-prompts-function-based","title":"3. Callable Prompts (Function-Based)","text":"<p>For maximum flexibility, you can register a function that generates the prompt dynamically:</p> <pre><code>def create_assistant_prompt(user_name: str, context: str = \"\") -&gt; str:\n    \"\"\"Generate a personalized assistant prompt.\"\"\"\n    base = f\"You are {user_name}'s assistant.\"\n    if context:\n        base += f\" Context: {context}\"\n    return base\n\nregistry = PromptRegistry()\nregistry.add_prompt(\"assistant\", create_assistant_prompt)\n\n# Render with arguments\nprompt = registry.render(\"assistant\", user_name=\"Bob\", context=\"coding session\")\n# Result: \"You are Bob's assistant. Context: coding session\"\n</code></pre>"},{"location":"how-to/#4-using-multiple-prompts-with-different-agents","title":"4. Using Multiple Prompts with Different Agents","text":"<p>The <code>PromptRegistry</code> integrates seamlessly with agents, allowing you to create multiple agents with different personas:</p> <pre><code>registry = PromptRegistry()\nregistry.add_prompt(\"coder\", \"You are an expert Python developer.\")\nregistry.add_prompt(\"writer\", \"You are a creative writing assistant.\")\n\n# Agent 1: Uses \"coder\" prompt\ndev_agent = Agent(\n    llm_client=client,\n    prompt_registry=registry,\n    system_prompt_name=\"coder\"\n)\n\n# Agent 2: Uses \"writer\" prompt\nwriter_agent = Agent(\n    llm_client=client,\n    prompt_registry=registry,\n    system_prompt_name=\"writer\"\n)\n</code></pre> <p>The agent automatically: 1. Looks up the prompt by <code>system_prompt_name</code> 2. Renders it (if it's a template or callable, you may need to provide kwargs) 3. Inserts it as the first message in memory (role: \"developer\")</p>"},{"location":"how-to/#5-advanced-use-cases","title":"5. Advanced Use Cases","text":""},{"location":"how-to/#multi-tenant-applications","title":"Multi-Tenant Applications","text":"<pre><code>def tenant_prompt(tenant_id: str, tenant_config: dict) -&gt; str:\n    \"\"\"Generate prompt based on tenant configuration.\"\"\"\n    style = tenant_config.get(\"style\", \"professional\")\n    domain = tenant_config.get(\"domain\", \"general\")\n    return f\"You are a {style} assistant specializing in {domain} for tenant {tenant_id}.\"\n\nregistry = PromptRegistry()\nregistry.add_prompt(\"tenant\", tenant_prompt)\n\n# Use in a web application\ndef get_agent_for_tenant(tenant_id: str):\n    config = get_tenant_config(tenant_id)\n    return Agent(\n        llm_client=client,\n        prompt_registry=registry,\n        system_prompt_name=\"tenant\"\n    )\n</code></pre>"},{"location":"how-to/#ab-testing-prompts","title":"A/B Testing Prompts","text":"<pre><code>registry = PromptRegistry()\nregistry.add_prompt(\"variant_a\", \"You are a concise assistant.\")\nregistry.add_prompt(\"variant_b\", \"You are a detailed, thorough assistant.\")\n\n# Switch between variants based on user\nvariant = \"variant_a\" if user_id % 2 == 0 else \"variant_b\"\nagent = Agent(..., prompt_registry=registry, system_prompt_name=variant)\n</code></pre>"},{"location":"how-to/#context-aware-prompts","title":"Context-Aware Prompts","text":"<pre><code>def contextual_prompt(current_time: str, user_location: str) -&gt; str:\n    return f\"\"\"You are a helpful assistant.\nCurrent time: {current_time}\nUser location: {user_location}\nAdjust your responses based on timezone and location.\"\"\"\n\nregistry = PromptRegistry()\nregistry.add_prompt(\"contextual\", contextual_prompt)\n\n# Render with current context\nfrom datetime import datetime\nprompt = registry.render(\n    \"contextual\",\n    current_time=datetime.now().isoformat(),\n    user_location=\"New York\"\n)\n</code></pre>"},{"location":"how-to/#best-practices","title":"Best Practices","text":"<ol> <li>Name Your Prompts Clearly: Use descriptive names like <code>\"customer_support\"</code> instead of <code>\"prompt1\"</code>.</li> <li>Keep Prompts Focused: Each prompt should define a single persona or role.</li> <li>Use Callables for Complex Logic: If you need database lookups, API calls, or complex string manipulation, use callable prompts.</li> <li>Document Your Prompts: Consider maintaining a separate file or docstring explaining what each prompt does.</li> <li>Version Your Prompts: For production systems, consider adding version numbers to prompt names (e.g., <code>\"assistant_v2\"</code>).</li> </ol>"},{"location":"how-to/#prompt-rendering-flow","title":"Prompt Rendering Flow","text":"<p>When an agent is initialized or when you manually render:</p> <pre><code># 1. Lookup\nprompt_template = registry.get_prompt(\"assistant\")\n\n# 2. Render (if needed)\nif callable(prompt_template):\n    rendered = prompt_template(**kwargs)  # Call the function\nelif isinstance(prompt_template, str) and kwargs:\n    rendered = prompt_template.format(**kwargs)  # Format string\nelse:\n    rendered = prompt_template  # Use as-is\n\n# 3. Use in agent memory\nagent.memory[0] = {\"role\": \"developer\", \"content\": rendered}\n</code></pre>"},{"location":"installation/","title":"Installation Guide","text":""},{"location":"installation/#system-requirements","title":"System Requirements","text":"<ul> <li>OS: macOS, Linux, or Windows</li> <li>Python: Version 3.10 or newer</li> </ul>"},{"location":"installation/#installing-via-pip","title":"Installing via pip","text":"<p>The easiest way to install Iris Agent is via <code>pip</code> from PyPI:</p> <pre><code>pip install iris-agent\n</code></pre>"},{"location":"installation/#installing-from-source","title":"Installing from Source","text":"<p>If you want the latest development version or want to contribute:</p> <ol> <li> <p>Clone the repository:     <pre><code>git clone https://github.com/mrgehlot/iris-agent.git\ncd iris-agent\n</code></pre></p> </li> <li> <p>Create a virtual environment (recommended):     <pre><code>python -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n</code></pre></p> </li> <li> <p>Install in editable mode:     <pre><code>pip install -e .\n</code></pre></p> </li> </ol>"},{"location":"installation/#installing-development-dependencies","title":"Installing Development Dependencies","text":"<p>If you plan to run tests or build documentation:</p> <pre><code>pip install -e \".[dev]\"\n</code></pre> <p>This installs tools like <code>pytest</code>, <code>black</code>, <code>isort</code>, and <code>mkdocs</code>.</p>"},{"location":"installation/#verifying-installation","title":"Verifying Installation","text":"<p>To check if the installation was successful, run a quick import in Python:</p> <pre><code>python -c \"import iris_agent; print(f'Iris Agent {iris_agent.__file__} installed successfully')\"\n</code></pre>"},{"location":"installation/#troubleshooting-installation","title":"Troubleshooting Installation","text":"<p>\"Module not found\" error: Ensure your virtual environment is activated.</p> <p>Asyncio errors: Iris Agent relies heavily on <code>asyncio</code>. Ensure you are not trying to nest <code>asyncio.run()</code> calls if you are using the synchronous <code>Agent</code> inside an already running event loop (e.g., in a Jupyter Notebook). For those cases, use <code>AsyncAgent</code>.</p>"},{"location":"modules/","title":"Modules Reference","text":"<p>The <code>iris_agent</code> package is structured as follows:</p>"},{"location":"modules/#srciris_agent","title":"<code>src/iris_agent/</code>","text":"<p>The core package source.</p> Module Description <code>agent.py</code> Contains the <code>Agent</code> class (Synchronous wrapper). <code>async_agent.py</code> Contains the <code>AsyncAgent</code> class (Core logic). <code>llm.py</code> Contains <code>BaseLLMClient</code>, <code>LLMConfig</code> and provider logic. <code>tools.py</code> Contains <code>ToolRegistry</code>, <code>@tool</code> decorator, and schema inference logic. <code>prompts.py</code> Contains <code>PromptRegistry</code> for managing system prompts. <code>messages.py</code> Helpers for creating messages (text, images). <code>types.py</code> Type definitions and constants (e.g., <code>Role</code>)."},{"location":"modules/#examples","title":"<code>examples/</code>","text":"<p>Contains reference implementations.</p> <ul> <li><code>01_basic/</code>: Simple chat and streaming examples.</li> <li><code>02_prompts/</code>: Using the prompt registry.</li> <li><code>03_tools/</code>: Tool creation and usage.</li> <li><code>04_memory/</code>: Memory inspection.</li> <li><code>05_multi_agent/</code>: Patterns for multiple agents.</li> <li><code>09_gemini/</code>: Specific examples for Google Gemini.</li> </ul>"},{"location":"modules/#tests","title":"<code>tests/</code>","text":"<p>Unit and integration tests using <code>pytest</code>.</p>"},{"location":"troubleshooting/","title":"Troubleshooting","text":""},{"location":"troubleshooting/#common-issues","title":"Common Issues","text":""},{"location":"troubleshooting/#1-runtimeerror-agentrun-called-inside-an-event-loop","title":"1. \"RuntimeError: Agent.run called inside an event loop\"","text":"<p>Cause: You are using the synchronous <code>Agent</code> class inside an async environment (like a Jupyter Notebook or a FastAPI route).</p> <p>Solution: Use <code>AsyncAgent</code> instead.</p> <pre><code># Bad\nagent = Agent(...)\nagent.run(\"hi\")\n\n# Good\nagent = AsyncAgent(...)\nawait agent.run(\"hi\")\n</code></pre>"},{"location":"troubleshooting/#2-tools-are-not-being-called","title":"2. Tools are not being called","text":"<p>Cause: - The tool might not be registered with the <code>ToolRegistry</code>. - The <code>ToolRegistry</code> was not passed to the <code>Agent</code> constructor. - The tool's docstring description is too vague for the LLM to understand when to use it.</p> <p>Solution: - Check registration: <code>print(registry.list_tools())</code>. - Improve docstrings: Explain what the tool does and when to use it.</p>"},{"location":"troubleshooting/#3-api-connection-errors","title":"3. API Connection Errors","text":"<p>Cause: Missing API key or incorrect Base URL.</p> <p>Solution: - Ensure <code>OPENAI_API_KEY</code> (or equivalent) is set. - If using a local model (e.g., Ollama), ensure <code>base_url</code> is correct (e.g., <code>http://localhost:11434/v1</code>).</p>"},{"location":"troubleshooting/#4-import-errors","title":"4. Import Errors","text":"<p>Cause: The package is not installed in the current environment.</p> <p>Solution: Run <code>pip install .</code> or <code>pip install -e .</code> in the project root.</p>"}]}