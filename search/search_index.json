{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Iris Agent Documentation","text":"<p>Welcome to the documentation for Iris Agent!</p> <p>Iris Agent is a lightweight, flexible, and provider-agnostic framework for building AI agents in Python. It simplifies the process of creating agents that can use tools, manage memory, and interact with various LLM providers.</p>"},{"location":"#motivation","title":"Motivation","text":"<p>In the rapidly evolving landscape of AI development, many frameworks have grown increasingly complex, introducing heavy abstractions, custom DSLs (Domain Specific Languages), and \"magic\" that can obscure what's actually happening under the hood. While powerful, these \"batteries-included\" approaches often lead to:</p> <ul> <li>Bloat: Including massive dependencies and logic for features you don't use.</li> <li>Opacity: Making it difficult to debug prompts or understand exactly what is being sent to the LLM.</li> <li>Rigidity: Locking you into specific orchestration patterns (chains, DAGs) that may not fit your use case.</li> </ul> <p>Iris Agent was built to be the antidote to framework fatigue. We encourage developers to build only what you need.</p>"},{"location":"#architectures-as-blueprints-for-thought","title":"Architectures as Blueprints for Thought","text":"<p>We believe that an agent's architecture is effectively the blueprint of its thought process. </p> <p>Different problems require different modes of cognition: - Simple Tasks: Might need a straightforward <code>Prompt -&gt; Response</code> chain. - Complex Reasoning: Might require a <code>ReAct</code> (Reason+Act) loop or <code>Plan-and-Solve</code> strategy. - Creative Work: Might benefit from a <code>Brainstorm -&gt; Critique -&gt; Refine</code> workflow.</p> <p>Frameworks that lock you into a specific \"Graph\" or \"Chain\" structure often constrain how you can model this decision-making process. Iris Agent provides the primitive components\u2014LLM clients, tool registries, memory stores\u2014and lets you determine the cognitive architecture. </p> <p>Whether you are building a simple chatbot or a complex multi-step reasoning agent, the flow of control remains in standard, readable Python code.</p>"},{"location":"#the-iris-difference-transparency-type-safety","title":"The Iris Difference: Transparency &amp; Type Safety","text":"<p>The key thing that separates Iris Agent from other major frameworks is its commitment to zero-overhead abstractions. </p> <p>Instead of wrapping simple API calls in complex \"Chains\" or \"Runnables\", Iris Agent exposes the raw power of Python: 1.  Standard Python Types: Tools are defined using standard Python type hints, not complex configuration objects. 2.  Direct Control: You have full visibility into the message history and prompt construction. 3.  No Hidden Magic: If you want to customize the loop, you can. The framework gets out of your way.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Provider Agnostic: Works with OpenAI, Google Gemini, or any OpenAI-compatible API (like LocalAI or vLLM).</li> <li>Tool Support: Easy-to-use <code>@tool</code> decorator that automatically infers JSON schemas from Python type hints.</li> <li>Async &amp; Sync: First-class async and sync agents.</li> <li>Streaming: Built-in support for streaming responses for real-time applications.</li> <li>Memory Management: Automatic conversation history management with support for custom memory stores.</li> <li>Prompt Management: Centralized <code>PromptRegistry</code> for reusable and template-based system prompts.</li> <li>Type Safety: Built with strict type hints for better developer experience and tooling support.</li> <li>Logging: Integrated Rich logging for beautiful, readable debug output.</li> </ul>"},{"location":"#architecture-overview","title":"Architecture Overview","text":"<p>Iris Agent is built around a few core components:</p> <pre><code>graph TD\n    A[Agent/AsyncAgent] --&gt; B[SyncLLMClient/AsyncLLMClient]\n    A --&gt; C[ToolRegistry]\n    A --&gt; D[PromptRegistry]\n    A --&gt; E[Memory List]\n    B --&gt; F[LLM Provider API]\n    C --&gt; G[Python Functions]</code></pre> <ul> <li>Agent: The central controller that orchestrates the LLM, tools, and memory.</li> <li>LLM Client: Handles communication with the AI provider.</li> <li>Tool Registry: Manages available tools and executes them when requested by the model.</li> <li>Prompt Registry: Stores system instructions and templates.</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<p>Here is a minimal example to get you running in seconds:</p> <pre><code>from iris_agent import Agent, LLMConfig, LLMProvider, PromptRegistry, SyncLLMClient\n\n# 1. Configure the LLM\nconfig = LLMConfig(\n    provider=LLMProvider.OPENAI,\n    model=\"gpt-4o\",\n    api_key=\"sk-...\"\n)\nclient = SyncLLMClient(config)\n\n# 2. Create a prompt registry and add a system prompt\nprompts = PromptRegistry()\nprompts.add_prompt(\"assistant\", \"You are a helpful AI assistant.\")\n\n# 3. Create the Agent with the system prompt\nagent = Agent(\n    llm_client=client,\n    prompt_registry=prompts,\n    system_prompt_name=\"assistant\"\n)\n\n# 4. Run\nresponse = agent.run(\"Hello! how are you doing today?\")\nprint(response)\n# I'm doing great! How about you?\n</code></pre>"},{"location":"#documentation-map","title":"Documentation Map","text":"<p>Explore the detailed documentation:</p> <ul> <li>Getting Started: Your first steps with Iris Agent.</li> <li>Installation: Setup guide for different environments.</li> <li>Core Concepts: Deep dive into Agents, Tools, and Memory.</li> <li>How-To Guides: Practical recipes for common tasks (Tools, Streaming, etc.).</li> <li>Modules Reference: Project structure overview.</li> <li>API Reference: Detailed class and function documentation.</li> <li>Examples: Code examples for various use cases.</li> <li>Troubleshooting: Solutions to common problems.</li> <li>FAQ: Frequently asked questions.</li> </ul>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the MIT License.</p>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/#agent","title":"Agent","text":""},{"location":"api/#iris_agentagent","title":"<code>iris_agent.Agent</code>","text":"<p>Synchronous agent for non-async usage.</p> <pre><code>class Agent:\n    def __init__(\n        self,\n        llm_client: SyncLLMClient,\n        prompt_registry: Optional[PromptRegistry] = None,\n        tool_registry: Optional[ToolRegistry] = None,\n        system_prompt_name: str = \"assistant\",\n        enable_logging: bool = False,\n        logger: Optional[logging.Logger] = None,\n    ) -&gt; None\n</code></pre> <p>Parameters:</p> <ul> <li><code>llm_client</code> (<code>SyncLLMClient</code>): The initialized sync LLM client instance.</li> <li><code>prompt_registry</code> (<code>PromptRegistry</code>, optional): Registry for system prompts. Defaults to a new empty registry.</li> <li><code>tool_registry</code> (<code>ToolRegistry</code>, optional): Registry for tools. Defaults to a new empty registry.</li> <li><code>system_prompt_name</code> (<code>str</code>, default=\"assistant\"): The key to look up in the prompt registry for the initial system message.</li> <li><code>enable_logging</code> (<code>bool</code>, default=False): If True, enables rich logging to stdout.</li> <li><code>logger</code> (<code>logging.Logger</code>, optional): Custom logger instance.</li> </ul>"},{"location":"api/#methods","title":"Methods","text":"<p><code>run(user_message: str | dict) -&gt; str</code></p> <p>Send a message to the agent and get a response.</p> <ul> <li><code>user_message</code>: The input text or a message dict (created via <code>create_message</code>).</li> <li>Returns: The assistant's response text.</li> </ul> <p><code>call_tool(name: str, **kwargs) -&gt; Any</code></p> <p>Manually call a tool registered with the agent.</p> <ul> <li><code>name</code>: Name of the tool.</li> <li><code>kwargs</code>: Arguments for the tool.</li> </ul>"},{"location":"api/#iris_agentasyncagent","title":"<code>iris_agent.AsyncAgent</code>","text":"<p>The core asynchronous agent class.</p> <pre><code>class AsyncAgent:\n    def __init__(\n        self,\n        llm_client: AsyncLLMClient,\n        prompt_registry: Optional[PromptRegistry] = None,\n        tool_registry: Optional[ToolRegistry] = None,\n        system_prompt_name: str = \"assistant\",\n        enable_logging: bool = False,\n        logger: Optional[logging.Logger] = None,\n    ) -&gt; None\n</code></pre> <p>Same parameters as <code>Agent</code>.</p>"},{"location":"api/#methods_1","title":"Methods","text":"<p><code>async run(user_message: str | dict, ...) -&gt; str</code></p> <p>Run a single turn of conversation.</p> <ul> <li><code>user_message</code>: Input text or dict.</li> <li><code>json_response</code> (<code>bool</code>): If True, requests JSON output from the LLM.</li> <li><code>max_completion_tokens</code> (<code>int</code>): Cap the response length.</li> <li><code>seed</code> (<code>int</code>): Deterministic sampling seed.</li> <li><code>reasoning_effort</code> (<code>str</code>): Reasoning effort for models that support it (e.g. \"high\", \"medium\", \"low\").</li> <li><code>web_search_options</code> (<code>dict</code>): Search options for models that support it.</li> <li><code>extra_body</code> (<code>dict</code>): Provider-specific request body overrides.</li> </ul> <p><code>async run_stream(user_message: str | dict, ...) -&gt; AsyncGenerator[str, None]</code></p> <p>Stream the response token by token. Automatically handles tool calls in the background during the stream.</p> <p><code>run_stream(user_message: str | dict, ...) -&gt; Generator[str, None, None]</code></p> <p>Sync streaming helper on <code>Agent</code> that yields chunks by driving the async stream under the hood. This cannot be called from within an active event loop.</p>"},{"location":"api/#llm-client","title":"LLM Client","text":""},{"location":"api/#iris_agentsyncllmclient","title":"<code>iris_agent.SyncLLMClient</code>","text":"<p>OpenAI-compatible sync client wrapper.</p> <pre><code>class SyncLLMClient:\n    def __init__(self, config: LLMConfig) -&gt; None\n</code></pre>"},{"location":"api/#iris_agentasyncllmclient","title":"<code>iris_agent.AsyncLLMClient</code>","text":"<p>OpenAI-compatible async client wrapper.</p> <pre><code>class AsyncLLMClient:\n    def __init__(self, config: LLMConfig) -&gt; None\n</code></pre>"},{"location":"api/#iris_agentbasellmclient","title":"<code>iris_agent.BaseLLMClient</code>","text":"<p>Shared helper base class for the sync and async clients.</p>"},{"location":"api/#iris_agentllmconfig","title":"<code>iris_agent.LLMConfig</code>","text":"<p>Configuration dataclass.</p> <pre><code>@dataclass\nclass LLMConfig:\n    provider: str\n    model: str\n    api_key: str | None = None\n    base_url: str | None = None\n    reasoning_effort: Optional[str] = None\n    web_search_options: Optional[dict] = None\n    extra_body: Optional[dict] = None\n</code></pre> <ul> <li><code>provider</code>: One of <code>LLMProvider</code> constants (e.g., <code>LLMProvider.OPENAI</code>).</li> <li><code>model</code>: Model identifier (e.g., \"gpt-4o\").</li> <li><code>base_url</code>: Optional override for compatible APIs.</li> <li><code>extra_body</code>: Optional provider-specific request body overrides.</li> </ul>"},{"location":"api/#tools","title":"Tools","text":""},{"location":"api/#iris_agenttool","title":"<code>iris_agent.tool</code>","text":"<p>Decorator to register a function as a tool.</p> <pre><code>@tool(name=\"custom_name\", description=\"Custom description\")\ndef my_function(arg: int): ...\n</code></pre>"},{"location":"api/#iris_agenttoolregistry","title":"<code>iris_agent.ToolRegistry</code>","text":"<p>Registry for managing tools.</p>"},{"location":"api/#methods_2","title":"Methods","text":"<p><code>register(func: Callable) -&gt; ToolSpec</code></p> <p>Register a single function.</p> <p><code>register_from(obj: Any) -&gt; None</code></p> <p>Scan an object (or module) for methods decorated with <code>@tool</code> and register them.</p>"},{"location":"api/#prompts","title":"Prompts","text":""},{"location":"api/#iris_agentpromptregistry","title":"<code>iris_agent.PromptRegistry</code>","text":"<p>Registry for managing system prompts.</p>"},{"location":"api/#methods_3","title":"Methods","text":"<p><code>add_prompt(name: str, template: str | Callable)</code></p> <p>Add a prompt.</p> <p><code>render(prompt_name: str, **kwargs) -&gt; str</code></p> <p>Render a prompt by name, passing kwargs to <code>format</code> or the callable.</p>"},{"location":"api/#messages","title":"Messages","text":""},{"location":"api/#iris_agentcreate_message","title":"<code>iris_agent.create_message</code>","text":"<p>Helper to create standard message dictionaries.</p> <pre><code>def create_message(\n    role: str,\n    content: str | None = None,\n    name: str | None = None,\n    images: List[str] | None = None\n) -&gt; dict\n</code></pre> <ul> <li><code>role</code>: \"user\", \"assistant\", \"system\", etc.</li> <li><code>content</code>: Text content.</li> <li><code>images</code>: List of image URLs.</li> </ul>"},{"location":"concepts/","title":"Core Concepts","text":"<p>Understanding how Iris Agent works under the hood will help you build more robust applications.</p>"},{"location":"concepts/#the-agent-loop","title":"The Agent Loop","text":"<p>At its core, an <code>Agent</code> (or <code>AsyncAgent</code>) implements a Loop that continues until the AI produces a final answer.</p> <pre><code>flowchart TD\n    Start([User Input]) --&gt; Receive[Receive Input]\n    Receive --&gt; UpdateMemory[Update Memory]\n    UpdateMemory --&gt; LLMCall[LLM Call&lt;br/&gt;Send history + tool definitions]\n    LLMCall --&gt; Decision{LLM Decision}\n    Decision --&gt;|Generate Response| Respond[Generate Text Response]\n    Decision --&gt;|Use Tool| CallTool[Call Tool&lt;br/&gt;Request function execution]\n    CallTool --&gt; Execute[Execute Tool&lt;br/&gt;Run Python function]\n    Execute --&gt; AddToolResult[Add Tool Result&lt;br/&gt;to Memory as Tool message]\n    AddToolResult --&gt; LLMCall\n    Respond --&gt; CheckToolCalls{More Tool Calls?}\n    CheckToolCalls --&gt;|Yes| CallTool\n    CheckToolCalls --&gt;|No| FinalOutput([Return Final Response])\n\n    style Start fill:#e1f5ff\n    style FinalOutput fill:#c8e6c9\n    style Decision fill:#fff9c4\n    style CheckToolCalls fill:#fff9c4</code></pre> <ol> <li>Receive Input: The agent accepts a user message (text or multimodal).</li> <li>Update Memory: The message is added to the agent's conversation history (<code>self.memory</code>).</li> <li>LLM Call: The agent sends the full history + available tool definitions to the LLM.</li> <li>Decision: The LLM decides to either:<ul> <li>Respond: Generate a text response.</li> <li>Call Tool: Request execution of a specific tool with specific arguments.</li> </ul> </li> <li>Execution (if Tool):<ul> <li>The agent executes the requested Python function.</li> <li>The result is captured and added to memory as a <code>Tool</code> message.</li> <li>The loop repeats (Go to Step 3).</li> </ul> </li> <li>Final Output: When the LLM generates a text response (and no more tool calls), the text is returned to the user.</li> </ol>"},{"location":"concepts/#memory","title":"Memory","text":"<p>Iris Agent uses a simple list-based memory structure compatible with OpenAI's chat format.</p> <ul> <li>System Message: The initial instruction (e.g., \"You are a helpful assistant\"). Always kept at index 0.</li> <li>User Message: Input from the human.</li> <li>Assistant Message: Output from the AI.</li> <li>Tool Message: Results from function calls.</li> </ul> <pre><code>[\n  {\"role\": \"developer\", \"content\": \"You are a helpful assistant.\"},\n  {\"role\": \"user\", \"content\": \"What is the date?\"},\n  {\"role\": \"assistant\", \"tool_calls\": [...]},\n  {\"role\": \"tool\", \"tool_call_id\": \"...\", \"content\": \"2023-10-27\"},\n  {\"role\": \"assistant\", \"content\": \"It is October 27, 2023.\"}\n]\n</code></pre>"},{"location":"concepts/#tools-and-type-inference","title":"Tools and Type Inference","text":"<p>One of the most powerful features of Iris Agent is how it handles tools. Instead of manually writing JSON schemas, you write standard Python functions with type hints.</p>"},{"location":"concepts/#the-tool-decorator","title":"The <code>@tool</code> Decorator","text":"<p>When you decorate a function with <code>@tool</code>, Iris Agent inspects the signature:</p> <pre><code>@tool\ndef calculate_tax(amount: float, rate: float = 0.1) -&gt; float:\n    \"\"\"Calculate tax for a given amount.\"\"\"\n    return amount * rate\n</code></pre> <p>This is automatically converted to: <pre><code>{\n  \"name\": \"calculate_tax\",\n  \"description\": \"Calculate tax for a given amount.\",\n  \"parameters\": {\n    \"type\": \"object\",\n    \"properties\": {\n      \"amount\": {\"type\": \"number\"},\n      \"rate\": {\"type\": \"number\"}\n    },\n    \"required\": [\"amount\"]\n  }\n}\n</code></pre></p> <p>The <code>ToolRegistry</code> handles this conversion and the subsequent execution validation.</p>"},{"location":"concepts/#registries","title":"Registries","text":"<p>To keep your code organized, Iris Agent uses Registries.</p> <ul> <li><code>ToolRegistry</code>: A collection of available tools. You can share registries between multiple agents.</li> <li><code>PromptRegistry</code>: A collection of system prompts or templates. This allows you to dynamically switch personas or update prompts without changing application code.</li> </ul>"},{"location":"concepts/#toolregistry","title":"ToolRegistry","text":"<p>The <code>ToolRegistry</code> manages tool registration, validation, and execution. You can register individual tools or scan entire objects for decorated tools.</p> <p>Basic Usage:</p> <pre><code>from iris_agent import ToolRegistry, tool\n\nregistry = ToolRegistry()\n\n@tool\ndef calculate_tax(amount: float, rate: float = 0.1) -&gt; float:\n    \"\"\"Calculate tax for a given amount.\"\"\"\n    return amount * rate\n\n# Register a single tool\nregistry.register(calculate_tax)\n\n# Get OpenAI-compatible schemas\nschemas = registry.schemas()\n\n# Call the tool directly\nresult = registry.call(\"calculate_tax\", amount=100.0, rate=0.15)\n</code></pre> <p>Registering Multiple Tools from an Object:</p> <pre><code>class MathTools:\n    @tool\n    def add(self, a: int, b: int) -&gt; int:\n        \"\"\"Add two numbers.\"\"\"\n        return a + b\n\n    @tool\n    def multiply(self, a: int, b: int) -&gt; int:\n        \"\"\"Multiply two numbers.\"\"\"\n        return a * b\n\nmath_tools = MathTools()\ntool_registry = ToolRegistry()\ntool_registry.register_from(math_tools)  # Registers both add and multiply\n\n# List all registered tools\nall_tools = tool_registry.list_tools()\n\n# Create an agent with the tool registry\nagent = Agent(llm_client=client, tool_registry=tool_registry)\n</code></pre> <p>Async Tools:</p> <pre><code>import httpx\n\n@tool\nasync def fetch_url(url: str) -&gt; str:\n    \"\"\"Fetch content from a URL.\"\"\"\n    async with httpx.AsyncClient() as client:\n        response = await client.get(url)\n        return response.text\n\nregistry.register(fetch_url)\n\n# Call async tool\nresult = await registry.call_async(\"fetch_url\", url=\"https://example.com\")\n</code></pre> <p>Sharing Registries Between Agents:</p> <pre><code># Create a shared tool registry\nshared_tools = ToolRegistry()\nshared_tools.register(calculate_tax)\nshared_tools.register(fetch_url)\n\n# Use with multiple agents\nagent1 = Agent(llm_client=client1, tool_registry=shared_tools)\nagent2 = Agent(llm_client=client2, tool_registry=shared_tools)\n</code></pre>"},{"location":"concepts/#promptregistry","title":"PromptRegistry","text":"<p>The <code>PromptRegistry</code> provides flexible prompt management with support for static strings, templates, and dynamic callable prompts.</p> <p>1. Static Prompts (Simple Strings):</p> <pre><code>from iris_agent import PromptRegistry, Agent, SyncLLMClient, LLMConfig, LLMProvider\n\nregistry = PromptRegistry()\nregistry.add_prompt(\"assistant\", \"You are a helpful AI assistant.\")\n\n# Get the prompt\nprompt = registry.get_prompt(\"assistant\")\n# Returns: \"You are a helpful AI assistant.\"\n\n# Create an agent with the prompt registry\nclient = SyncLLMClient(LLMConfig(provider=LLMProvider.OPENAI, model=\"gpt-4o-mini\", api_key=\"...\"))\nagent = Agent(\n    llm_client=client,\n    prompt_registry=registry,\n    system_prompt_name=\"assistant\"\n)\n\n# Use the agent\nresponse = agent.run(\"Hello, how are you?\")\n</code></pre> <p>2. Template Prompts (String Formatting):</p> <p>Template prompts use Python's <code>.format()</code> syntax for variable substitution. Since agents render prompts without arguments, you have two options:</p> <p>Option A: Pre-render and add to registry</p> <pre><code>from iris_agent import PromptRegistry, Agent, SyncLLMClient, LLMConfig, LLMProvider\n\nregistry = PromptRegistry()\nregistry.add_prompt(\n    \"personal_assistant\",\n    \"You are {name}'s personal assistant. Today is {date}.\"\n)\n\n# Render with variables\nrendered = registry.render(\"personal_assistant\", name=\"Alice\", date=\"2024-01-15\")\n# Returns: \"You are Alice's personal assistant. Today is 2024-01-15.\"\n\n# Add the rendered prompt back to the registry with a new name\nregistry.add_prompt(\"alice_assistant\", rendered)\n\n# Create agent with the rendered prompt\nagent = Agent(\n    llm_client=client,\n    prompt_registry=registry,\n    system_prompt_name=\"alice_assistant\"\n)\n\n# Use the agent\nresponse = agent.run(\"What can you help me with today?\")\n</code></pre> <p>Option B: Use callable prompts (recommended for templates with variables)</p> <p>For templates that need variables, callable prompts are more flexible:</p> <pre><code>from iris_agent import PromptRegistry, Agent, SyncLLMClient, LLMConfig, LLMProvider\n\ndef personal_assistant_prompt(name: str = \"User\", date: str = \"\") -&gt; str:\n    \"\"\"Generate personalized assistant prompt.\"\"\"\n    date_str = f\" Today is {date}.\" if date else \"\"\n    return f\"You are {name}'s personal assistant.{date_str}\"\n\nregistry = PromptRegistry()\nregistry.add_prompt(\"personal_assistant\", personal_assistant_prompt)\n\n# Create client\nclient = SyncLLMClient(LLMConfig(provider=LLMProvider.OPENAI, model=\"gpt-4o-mini\", api_key=\"...\"))\n\n# Option 1: Use with defaults (if function has defaults)\n# Since this function has defaults, it will work without arguments\nagent = Agent(\n    llm_client=client,\n    prompt_registry=registry,\n    system_prompt_name=\"personal_assistant\"\n)\n\n# Option 2: Render with specific values and add to registry\nrendered = registry.render(\"personal_assistant\", name=\"Alice\", date=\"2024-01-15\")\nregistry.add_prompt(\"alice_assistant\", rendered)\nagent = Agent(\n    llm_client=client,\n    prompt_registry=registry,\n    system_prompt_name=\"alice_assistant\"\n)\n\n# Use the agent\nresponse = agent.run(\"What can you help me with?\")\n</code></pre> <p>3. Callable Prompts (Function-Based):</p> <p>For maximum flexibility, register functions that generate prompts dynamically:</p> <pre><code>from iris_agent import PromptRegistry, Agent, SyncLLMClient, LLMConfig, LLMProvider\n\ndef create_assistant_prompt(user_name: str, context: str = \"\") -&gt; str:\n    \"\"\"Generate a personalized assistant prompt.\"\"\"\n    base = f\"You are {user_name}'s assistant.\"\n    if context:\n        base += f\" Context: {context}\"\n    return base\n\nregistry = PromptRegistry()\nregistry.add_prompt(\"assistant\", create_assistant_prompt)\n\n# Option 1: Use with default arguments (if function has defaults)\n# Since create_assistant_prompt requires user_name, we need to pre-render\n\n# Render with function arguments\nprompt = registry.render(\"assistant\", user_name=\"Bob\", context=\"coding session\")\n# Returns: \"You are Bob's assistant. Context: coding session\"\n\n# Add the rendered prompt to registry\nregistry.add_prompt(\"bob_assistant\", prompt)\n\n# Create agent with the rendered prompt\nagent = Agent(\n    llm_client=client,\n    prompt_registry=registry,\n    system_prompt_name=\"bob_assistant\"\n)\n\n# Now use the agent\nresponse = agent.run(\"What can you help me with?\")\n</code></pre> <p>Better approach: Use callable with defaults</p> <pre><code>def create_assistant_prompt(user_name: str = \"User\", context: str = \"\") -&gt; str:\n    \"\"\"Generate a personalized assistant prompt.\"\"\"\n    base = f\"You are {user_name}'s assistant.\"\n    if context:\n        base += f\" Context: {context}\"\n    return base\n\nregistry = PromptRegistry()\nregistry.add_prompt(\"assistant\", create_assistant_prompt)\n\n# Now it works directly with agent (uses defaults)\nagent = Agent(\n    llm_client=client,\n    prompt_registry=registry,\n    system_prompt_name=\"assistant\"\n)\n\n# Or render with specific values and add to registry\nrendered = registry.render(\"assistant\", user_name=\"Bob\", context=\"coding session\")\nregistry.add_prompt(\"bob_assistant\", rendered)\nagent = Agent(\n    llm_client=client,\n    prompt_registry=registry,\n    system_prompt_name=\"bob_assistant\"\n)\n</code></pre> <p>4. Context-Aware Dynamic Prompts:</p> <pre><code>from datetime import datetime\nfrom iris_agent import PromptRegistry, Agent, SyncLLMClient, LLMConfig, LLMProvider\n\ndef contextual_prompt(current_time: str, user_location: str) -&gt; str:\n    return f\"\"\"You are a helpful assistant.\nCurrent time: {current_time}\nUser location: {user_location}\nAdjust your responses based on timezone and location.\"\"\"\n\nregistry = PromptRegistry()\nregistry.add_prompt(\"contextual\", contextual_prompt)\n\n# Render with current context\nprompt = registry.render(\n    \"contextual\",\n    current_time=datetime.now().isoformat(),\n    user_location=\"New York\"\n)\n\n# Add the rendered prompt to registry\nregistry.add_prompt(\"contextual_ny\", prompt)\n\n# Create agent with the context-aware prompt\nagent = Agent(\n    llm_client=client,\n    prompt_registry=registry,\n    system_prompt_name=\"contextual_ny\"\n)\n\n# Use the agent - it will have context about time and location\nresponse = agent.run(\"What's the weather like?\")\n</code></pre> <p>5. Multiple Personas:</p> <pre><code>from iris_agent import PromptRegistry, Agent, SyncLLMClient, LLMConfig, LLMProvider\n\nregistry = PromptRegistry()\nregistry.add_prompt(\"friendly\", \"You are a friendly and cheerful assistant.\")\nregistry.add_prompt(\"professional\", \"You are a professional business assistant.\")\nregistry.add_prompt(\"pirate\", \"You are a friendly pirate assistant. Arr!\")\n\n# Create client\nclient = SyncLLMClient(LLMConfig(provider=LLMProvider.OPENAI, model=\"gpt-4o-mini\", api_key=\"...\"))\n\n# Switch personas dynamically\npersona = \"pirate\"  # Could come from user preference\nagent = Agent(\n    llm_client=client,\n    prompt_registry=registry,\n    system_prompt_name=persona\n)\n\n# Use the agent with the selected persona\nresponse = agent.run(\"Tell me about yourself.\")\n</code></pre> <p>6. Template with Conditional Logic:</p> <pre><code>from iris_agent import PromptRegistry, Agent, SyncLLMClient, LLMConfig, LLMProvider\n\ndef smart_prompt(user_level: str, language: str = \"en\") -&gt; str:\n    \"\"\"Generate prompt based on user expertise level.\"\"\"\n    if user_level == \"beginner\":\n        instruction = \"Explain concepts in simple terms with examples.\"\n    elif user_level == \"expert\":\n        instruction = \"Use technical terminology and advanced concepts.\"\n    else:\n        instruction = \"Provide balanced explanations.\"\n\n    lang_note = f\" Respond in {language}.\" if language != \"en\" else \"\"\n    return f\"You are a helpful assistant.{lang_note} {instruction}\"\n\nregistry = PromptRegistry()\nregistry.add_prompt(\"smart\", smart_prompt)\n\n# Render for different users\nbeginner_prompt = registry.render(\"smart\", user_level=\"beginner\", language=\"en\")\nexpert_prompt = registry.render(\"smart\", user_level=\"expert\", language=\"es\")\n\n# Add rendered prompts to registry\nregistry.add_prompt(\"beginner_assistant\", beginner_prompt)\nregistry.add_prompt(\"expert_assistant_es\", expert_prompt)\n\n# Create agents for different user types\nclient = SyncLLMClient(LLMConfig(provider=LLMProvider.OPENAI, model=\"gpt-4o-mini\", api_key=\"...\"))\n\n# Agent for beginners\nbeginner_agent = Agent(\n    llm_client=client,\n    prompt_registry=registry,\n    system_prompt_name=\"beginner_assistant\"\n)\n\n# Agent for experts (Spanish)\nexpert_agent = Agent(\n    llm_client=client,\n    prompt_registry=registry,\n    system_prompt_name=\"expert_assistant_es\"\n)\n\n# Use the agents\nbeginner_response = beginner_agent.run(\"What is machine learning?\")\nexpert_response = expert_agent.run(\"Explain transformer architecture.\")\n</code></pre> <p>7. Retrieving and Checking Prompts:</p> <pre><code>registry = PromptRegistry()\nregistry.add_prompt(\"test\", \"Test prompt\")\n\n# Get prompt (returns None if not found)\nprompt = registry.get_prompt(\"test\")  # Returns: \"Test prompt\"\nmissing = registry.get_prompt(\"nonexistent\")  # Returns: None\n\n# Render (returns None if prompt not found)\nrendered = registry.render(\"test\")  # Returns: \"Test prompt\"\nmissing_rendered = registry.render(\"nonexistent\")  # Returns: None\n</code></pre> <p>Key Points: - Static prompts: Work directly with agents - Template prompts with variables: Must be pre-rendered and added back to registry, or use callable prompts - Callable prompts: Work best if they have default arguments, otherwise pre-render them</p>"},{"location":"concepts/#llm-client-architecture","title":"LLM Client Architecture","text":"<p>The <code>SyncLLMClient</code> and <code>AsyncLLMClient</code> are the interface between your agent and the AI provider. They share common behavior via <code>BaseLLMClient</code>.</p>"},{"location":"concepts/#llmconfig","title":"LLMConfig","text":"<p>The <code>LLMConfig</code> dataclass contains all the information needed to connect to an LLM:</p> <ul> <li><code>provider</code>: The provider identifier (e.g., <code>LLMProvider.OPENAI</code>, <code>LLMProvider.GOOGLE</code>)</li> <li><code>model</code>: The specific model name (e.g., <code>\"gpt-4o\"</code>, <code>\"gpt-4o-mini\"</code>)</li> <li><code>api_key</code>: Your API key (can be <code>None</code> if using environment variables)</li> <li><code>base_url</code>: Optional override for custom endpoints (useful for local models, proxies, or Gemini via OpenAI-compatible API)</li> <li><code>reasoning_effort</code>: Optional hint for models that support reasoning (e.g., <code>\"high\"</code>, <code>\"medium\"</code>, <code>\"low\"</code>)</li> <li><code>web_search_options</code>: Optional configuration for models that support web search</li> <li><code>extra_body</code>: Optional provider-specific request overrides (e.g., Gemini <code>thinking_config</code>)</li> </ul>"},{"location":"concepts/#provider-compatibility","title":"Provider Compatibility","text":"<p>The LLM clients use the OpenAI SDK under the hood, which means they work with: - OpenAI: Direct API access - Google Gemini: Via OpenAI-compatible base URL (<code>https://generativelanguage.googleapis.com/v1beta/openai/</code>) - Local Models: Any OpenAI-compatible API (Ollama, vLLM, LocalAI, etc.)</p> <p>The client automatically handles differences in API capabilities (e.g., JSON mode support detection).</p>"},{"location":"concepts/#messages-and-roles","title":"Messages and Roles","text":"<p>Iris Agent uses a message-based conversation format compatible with OpenAI's chat completions API.</p>"},{"location":"concepts/#message-structure","title":"Message Structure","text":"<p>Messages are dictionaries with a <code>role</code> field and optional <code>content</code>, <code>name</code>, <code>images</code>, and <code>tool_calls</code> fields:</p> <pre><code>{\n  \"role\": \"user\",\n  \"content\": \"Hello!\",\n  \"name\": \"Alice\",  # Optional: for multi-user conversations\n  \"images\": [...]   # Optional: for multimodal messages\n}\n</code></pre>"},{"location":"concepts/#roles","title":"Roles","text":"<p>The framework supports five roles (defined in <code>Role</code> class):</p> <ul> <li><code>SYSTEM</code> / <code>DEVELOPER</code>: System instructions. The agent uses <code>DEVELOPER</code> by default for prompts from <code>PromptRegistry</code>. Both are treated identically and always kept at index 0 in memory.</li> <li><code>USER</code>: Human input messages.</li> <li><code>ASSISTANT</code>: AI-generated responses. Can include <code>tool_calls</code> when the model wants to use tools.</li> <li><code>TOOL</code>: Results from function calls. Must include <code>tool_call_id</code> to link back to the original request.</li> </ul>"},{"location":"concepts/#creating-messages","title":"Creating Messages","text":"<p>Use the <code>create_message()</code> helper function to create properly formatted messages:</p> <pre><code>from iris_agent import create_message, Role\n\n# Simple text message\nuser_msg = create_message(Role.USER, \"What's the weather?\")\n\n# Multimodal message with images\nimage_msg = create_message(\n    role=Role.USER,\n    content=\"Describe this image\",\n    images=[\"https://example.com/photo.jpg\"]\n)\n\n# Image-only message (no text)\nimage_only = create_message(\n    role=Role.USER,\n    content=\"\",  # Empty string for image-only\n    images=[\"https://example.com/chart.png\"]\n)\n</code></pre>"},{"location":"concepts/#tool-execution-and-error-handling","title":"Tool Execution and Error Handling","text":"<p>When the LLM requests a tool call, the agent:</p> <ol> <li>Parses Arguments: Extracts JSON arguments from the tool call</li> <li>Validates: Checks arguments against the tool's schema (type checking, required fields, enum values)</li> <li>Executes: Calls the Python function (sync or async)</li> <li>Handles Errors: If an exception occurs, the error message is captured and sent back to the LLM as the tool response, allowing the model to retry or adjust</li> </ol> <pre><code># If a tool raises an exception:\ntry:\n    result = await tool_registry.call_async(\"get_weather\", location=\"Tokyo\")\nexcept Exception as exc:\n    # The agent automatically wraps this as:\n    # {\"role\": \"tool\", \"content\": f\"Tool error: {exc}\"}\n</code></pre> <p>This error handling allows the LLM to understand what went wrong and potentially try a different approach.</p>"},{"location":"concepts/#streaming-architecture","title":"Streaming Architecture","text":"<p>When using <code>run_stream()</code>, the agent handles streaming differently from <code>run()</code>:</p> <ol> <li>Stream Collection: Instead of waiting for the full response, chunks are yielded immediately to the caller</li> <li>Tool Call Buffering: Tool calls arrive incrementally in the stream. The agent buffers these chunks until the stream completes</li> <li>Tool Execution: After the stream ends, if tool calls were detected, they are executed (just like in <code>run()</code>)</li> <li>Loop Continuation: The agent continues the loop, sending tool results back and streaming the next response</li> </ol> <p>This allows for real-time user feedback while still supporting the full tool-calling workflow.</p>"},{"location":"concepts/#advanced-llm-parameters","title":"Advanced LLM Parameters","text":"<p>The agent supports several advanced parameters that can be passed to <code>run()</code> or <code>run_stream()</code>:</p> <ul> <li><code>json_response</code>: Forces the model to output valid JSON only (requires model support)</li> <li><code>seed</code>: Sets a random seed for deterministic outputs (useful for testing)</li> <li><code>max_completion_tokens</code> / <code>max_tokens</code>: Limits the response length</li> <li><code>reasoning_effort</code>: Hints to models like o1 about how much reasoning to perform</li> <li><code>web_search_options</code>: Enables web search for models that support it</li> </ul> <p>These parameters are passed through to the underlying LLM client and may not be supported by all providers.</p>"},{"location":"concepts/#logging","title":"Logging","text":"<p>Iris Agent includes built-in support for Rich logging, which provides beautiful, colorized terminal output showing:</p> <ul> <li>User messages (cyan)</li> <li>LLM calls (dim)</li> <li>Tool calls (magenta)</li> <li>Tool responses (green)</li> <li>Assistant responses (bold green)</li> <li>Finish reasons (dim)</li> </ul> <p>Enable logging by setting <code>enable_logging=True</code> when creating an agent:</p> <pre><code>agent = Agent(..., enable_logging=True)\n</code></pre> <p>You can also provide a custom logger instance for integration with your existing logging infrastructure.</p>"},{"location":"concepts/#sync-vs-async","title":"Sync vs Async","text":"<ul> <li><code>AsyncAgent</code>: Async agent that uses <code>asyncio</code> for non-blocking I/O (HTTP requests to LLMs, database calls in tools). Ideal for web servers (FastAPI, Quart). Supports both <code>run()</code> and <code>run_stream()</code>.</li> <li><code>Agent</code>: Synchronous agent that uses the sync client. Ideal for scripts, CLI tools, or data science notebooks where <code>async/await</code> syntax might be cumbersome. Supports both <code>run()</code> and <code>run_stream()</code>.</li> </ul>"},{"location":"examples/","title":"Examples","text":"<p>This page contains complete, runnable examples demonstrating how to use Iris Agent. All examples are located in the <code>examples/</code> directory of the repository.</p>"},{"location":"examples/#quick-navigation","title":"Quick Navigation","text":"<ul> <li>Basic Usage</li> <li>Prompts</li> <li>Tools</li> <li>Memory</li> <li>Multi-Agent</li> <li>Streaming</li> <li>Advanced Features</li> </ul>"},{"location":"examples/#basic-usage","title":"Basic Usage","text":""},{"location":"examples/#simple-synchronous-agent","title":"Simple Synchronous Agent","text":"<p>The most basic example - create an agent and have a conversation.</p> <p>File: <code>examples/01_basic/simple_agent.py</code></p> <pre><code>#!/usr/bin/env python3\nimport os\nfrom iris_agent import Agent, SyncLLMClient, LLMConfig, LLMProvider, PromptRegistry\n\ndef main() -&gt; int:\n    api_key = os.getenv(\"OPENAI_API_KEY\")\n    model = os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\")\n    base_url = os.getenv(\"OPENAI_BASE_URL\")\n\n    if not api_key:\n        print(\"Set OPENAI_API_KEY before running this example.\")\n        return 1\n\n    prompts = PromptRegistry()\n    prompts.add_prompt(\"assistant\", \"You are a concise assistant.\")\n\n    llm_config = LLMConfig(\n        provider=LLMProvider.OPENAI,\n        model=model,\n        api_key=api_key,\n        base_url=base_url,\n    )\n    client = SyncLLMClient(llm_config)\n\n    agent = Agent(llm_client=client, prompt_registry=prompts)\n    response = agent.run(\"Say hello in one sentence.\")\n    print(response)\n    return 0\n\nif __name__ == \"__main__\":\n    raise SystemExit(main())\n</code></pre>"},{"location":"examples/#async-agent","title":"Async Agent","text":"<p>Use <code>AsyncAgent</code> for async/await workflows, ideal for web servers.</p> <p>File: <code>examples/01_basic/async_agent.py</code></p> <pre><code>#!/usr/bin/env python3\nimport asyncio\nimport os\nfrom iris_agent import AsyncAgent, AsyncLLMClient, LLMConfig, LLMProvider, PromptRegistry\n\nasync def main() -&gt; int:\n    api_key = os.getenv(\"OPENAI_API_KEY\")\n    model = os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\")\n    base_url = os.getenv(\"OPENAI_BASE_URL\")\n\n    if not api_key:\n        print(\"Set OPENAI_API_KEY before running this example.\")\n        return 1\n\n    prompts = PromptRegistry()\n    prompts.add_prompt(\"assistant\", \"You are a helpful assistant.\")\n\n    llm_config = LLMConfig(\n        provider=LLMProvider.OPENAI,\n        model=model,\n        api_key=api_key,\n        base_url=base_url,\n    )\n    client = AsyncLLMClient(llm_config)\n\n    agent = AsyncAgent(llm_client=client, prompt_registry=prompts)\n    response = await agent.run(\"Explain what an async function is in one sentence.\")\n    print(response)\n    return 0\n\nif __name__ == \"__main__\":\n    raise SystemExit(asyncio.run(main()))\n</code></pre>"},{"location":"examples/#streaming","title":"Streaming","text":""},{"location":"examples/#async-streaming","title":"Async Streaming","text":"<p>Stream responses in real-time with <code>AsyncAgent.run_stream()</code>.</p> <p>File: <code>examples/01_basic/streaming_agent.py</code></p> <pre><code>#!/usr/bin/env python3\nimport asyncio\nimport os\nfrom iris_agent import AsyncAgent, AsyncLLMClient, LLMConfig, LLMProvider, PromptRegistry\n\nasync def main() -&gt; int:\n    api_key = os.getenv(\"OPENAI_API_KEY\")\n    model = os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\")\n    base_url = os.getenv(\"OPENAI_BASE_URL\")\n\n    if not api_key:\n        print(\"Set OPENAI_API_KEY before running this example.\")\n        return 1\n\n    prompts = PromptRegistry()\n    prompts.add_prompt(\"assistant\", \"You are a friendly assistant.\")\n\n    llm_config = LLMConfig(\n        provider=LLMProvider.OPENAI,\n        model=model,\n        api_key=api_key,\n        base_url=base_url,\n    )\n    client = AsyncLLMClient(llm_config)\n\n    agent = AsyncAgent(llm_client=client, prompt_registry=prompts)\n    async for chunk in agent.run_stream(\"Write a 3-sentence story about a robot.\"):\n        print(chunk, end=\"\", flush=True)\n    print()\n    return 0\n\nif __name__ == \"__main__\":\n    raise SystemExit(asyncio.run(main()))\n</code></pre>"},{"location":"examples/#sync-streaming","title":"Sync Streaming","text":"<p>Use <code>Agent.run_stream()</code> for synchronous streaming.</p> <p>File: <code>examples/01_basic/sync_streaming_agent.py</code></p> <pre><code>#!/usr/bin/env python3\nimport os\nfrom iris_agent import Agent, SyncLLMClient, LLMConfig, LLMProvider\n\ndef main() -&gt; int:\n    api_key = os.getenv(\"OPENAI_API_KEY\")\n    model = os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\")\n    base_url = os.getenv(\"OPENAI_BASE_URL\")\n\n    if not api_key:\n        print(\"Set OPENAI_API_KEY before running this example.\")\n        return 1\n\n    config = LLMConfig(\n        provider=LLMProvider.OPENAI,\n        model=model,\n        api_key=api_key,\n        base_url=base_url,\n    )\n    client = SyncLLMClient(config)\n    agent = Agent(llm_client=client)\n\n    for chunk in agent.run_stream(\"Tell me a short story about a robot.\"):\n        print(chunk, end=\"\", flush=True)\n    print()\n    return 0\n\nif __name__ == \"__main__\":\n    raise SystemExit(main())\n</code></pre>"},{"location":"examples/#prompts","title":"Prompts","text":""},{"location":"examples/#static-prompts","title":"Static Prompts","text":"<p>Register and use simple string prompts.</p> <p>File: <code>examples/02_prompts/prompt_registry_basic.py</code></p> <pre><code>#!/usr/bin/env python3\nfrom iris_agent import PromptRegistry\n\ndef main() -&gt; int:\n    prompts = PromptRegistry()\n    prompts.add_prompt(\"greeting\", \"Hello {name}!\")\n\n    print(prompts.render(\"greeting\", name=\"Iris\"))\n    print(prompts.render(\"missing\") or \"No prompt found.\")\n    return 0\n\nif __name__ == \"__main__\":\n    raise SystemExit(main())\n</code></pre>"},{"location":"examples/#dynamic-prompts","title":"Dynamic Prompts","text":"<p>Use callable functions to generate prompts dynamically.</p> <p>File: <code>examples/02_prompts/prompt_registry_dynamic.py</code></p> <pre><code>#!/usr/bin/env python3\nfrom iris_agent import PromptRegistry\n\ndef main() -&gt; int:\n    prompts = PromptRegistry()\n\n    def assistant_for(name: str) -&gt; str:\n        return f\"You are {name}'s assistant. Be concise.\"\n\n    prompts.add_prompt(\"assistant\", assistant_for)\n    print(prompts.render(\"assistant\", name=\"Abhishek\"))\n    return 0\n\nif __name__ == \"__main__\":\n    raise SystemExit(main())\n</code></pre>"},{"location":"examples/#tools","title":"Tools","text":""},{"location":"examples/#basic-tool-registration","title":"Basic Tool Registration","text":"<p>Register a simple function as a tool.</p> <p>File: <code>examples/03_tools/tool_registry_basic.py</code></p> <pre><code>#!/usr/bin/env python3\nfrom iris_agent import ToolRegistry, tool\n\ndef main() -&gt; int:\n    registry = ToolRegistry()\n\n    @tool(description=\"Add two numbers\")\n    def add(a: int, b: int) -&gt; int:\n        return a + b\n\n    registry.register(add)\n    print(\"Schemas:\", registry.schemas())\n    print(\"add(2, 3) =\", registry.call(\"add\", a=2, b=3))\n    return 0\n\nif __name__ == \"__main__\":\n    raise SystemExit(main())\n</code></pre>"},{"location":"examples/#using-tools-with-agent","title":"Using Tools with Agent","text":"<p>Create an agent that can use tools to perform actions.</p> <p>File: <code>examples/03_tools/tool_agent_usage.py</code></p> <pre><code>#!/usr/bin/env python3\nimport os\nfrom iris_agent import Agent, SyncLLMClient, LLMConfig, LLMProvider, PromptRegistry, ToolRegistry, tool\n\ndef main() -&gt; int:\n    api_key = os.getenv(\"OPENAI_API_KEY\")\n    model = os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\")\n    base_url = os.getenv(\"OPENAI_BASE_URL\")\n\n    if not api_key:\n        print(\"Set OPENAI_API_KEY before running this example.\")\n        return 1\n\n    prompts = PromptRegistry()\n    prompts.add_prompt(\"assistant\", \"You are a helpful math assistant.\")\n\n    tools = ToolRegistry()\n\n    @tool(description=\"Add two numbers\")\n    def add(a: int, b: int) -&gt; int:\n        return a + b\n\n    tools.register(add)\n\n    llm_config = LLMConfig(\n        provider=LLMProvider.OPENAI,\n        model=model,\n        api_key=api_key,\n        base_url=base_url,\n    )\n    client = SyncLLMClient(llm_config)\n\n    agent = Agent(llm_client=client, prompt_registry=prompts, tool_registry=tools)\n    response = agent.run(\"What is 12 + 30? Use the add tool.\")\n    print(response)\n    return 0\n\nif __name__ == \"__main__\":\n    raise SystemExit(main())\n</code></pre>"},{"location":"examples/#async-tools","title":"Async Tools","text":"<p>Define and use async tools for I/O operations.</p> <p>File: <code>examples/03_tools/async_tools.py</code></p> <pre><code>#!/usr/bin/env python3\nimport asyncio\nfrom iris_agent import ToolRegistry, tool\n\nasync def main() -&gt; int:\n    registry = ToolRegistry()\n\n    @tool(description=\"Async add\")\n    async def add_async(a: int, b: int) -&gt; int:\n        return a + b\n\n    registry.register(add_async)\n    result = await registry.call_async(\"add_async\", a=5, b=7)\n    print(\"add_async(5, 7) =\", result)\n    return 0\n\nif __name__ == \"__main__\":\n    raise SystemExit(asyncio.run(main()))\n</code></pre>"},{"location":"examples/#custom-tool-schema","title":"Custom Tool Schema","text":"<p>Override automatic schema inference with a custom schema.</p> <p>File: <code>examples/03_tools/tool_schema_custom.py</code></p> <pre><code>#!/usr/bin/env python3\nfrom iris_agent import ToolRegistry, tool\n\ndef main() -&gt; int:\n    registry = ToolRegistry()\n\n    @tool(\n        name=\"search_web\",\n        description=\"Search the web for a query.\",\n        parameters={\n            \"type\": \"object\",\n            \"properties\": {\"query\": {\"type\": \"string\"}},\n            \"required\": [\"query\"],\n        },\n    )\n    def search_web(query: str) -&gt; str:\n        return f\"Results for: {query}\"\n\n    registry.register(search_web)\n    print(\"Schemas:\", registry.schemas())\n    print(\"search_web:\", registry.call(\"search_web\", query=\"iris agent\"))\n    return 0\n\nif __name__ == \"__main__\":\n    raise SystemExit(main())\n</code></pre>"},{"location":"examples/#memory","title":"Memory","text":""},{"location":"examples/#memory-basics","title":"Memory Basics","text":"<p>Inspect, seed, and clear agent memory.</p> <p>File: <code>examples/04_memory/memory_basics.py</code></p> <pre><code>#!/usr/bin/env python3\nimport os\nfrom iris_agent import Agent, SyncLLMClient, LLMConfig, LLMProvider, PromptRegistry, Role, create_message\n\ndef main() -&gt; int:\n    api_key = os.getenv(\"OPENAI_API_KEY\") or \"dummy\"\n    model = os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\")\n    base_url = os.getenv(\"OPENAI_BASE_URL\")\n\n    prompts = PromptRegistry()\n    prompts.add_prompt(\"assistant\", \"You are a helpful assistant.\")\n\n    llm_config = LLMConfig(\n        provider=LLMProvider.OPENAI,\n        model=model,\n        api_key=api_key,\n        base_url=base_url,\n    )\n    client = SyncLLMClient(llm_config)\n\n    agent = Agent(llm_client=client, prompt_registry=prompts)\n\n    print(\"Initial memory:\", agent.memory)\n    agent.memory.append(create_message(Role.USER, \"Seeded message\"))\n    print(\"After seeding:\", agent.memory)\n\n    if os.getenv(\"OPENAI_API_KEY\"):\n        response = agent.run(\"Reply to the seeded message.\")\n        print(\"Assistant response:\", response)\n        print(\"Final memory size:\", len(agent.memory))\n    else:\n        print(\"Set OPENAI_API_KEY to run the agent and see memory growth.\")\n\n    agent.memory.clear()\n    print(\"After clear:\", agent.memory)\n    return 0\n\nif __name__ == \"__main__\":\n    raise SystemExit(main())\n</code></pre>"},{"location":"examples/#multi-agent","title":"Multi-Agent","text":""},{"location":"examples/#two-agents-chatting","title":"Two Agents Chatting","text":"<p>Two agents with different personas having a conversation.</p> <p>File: <code>examples/05_multi_agent/two_agents_chat.py</code></p> <pre><code>#!/usr/bin/env python3\nimport os\nfrom iris_agent import Agent, SyncLLMClient, LLMConfig, LLMProvider, PromptRegistry\n\ndef main() -&gt; int:\n    api_key = os.getenv(\"OPENAI_API_KEY\")\n    model = os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\")\n    base_url = os.getenv(\"OPENAI_BASE_URL\")\n\n    if not api_key:\n        print(\"Set OPENAI_API_KEY before running this example.\")\n        return 1\n\n    prompts_a = PromptRegistry()\n    prompts_a.add_prompt(\"assistant\", \"You are Agent A. Be concise.\")\n\n    prompts_b = PromptRegistry()\n    prompts_b.add_prompt(\"assistant\", \"You are Agent B. Ask clarifying questions.\")\n\n    llm_config = LLMConfig(\n        provider=LLMProvider.OPENAI,\n        model=model,\n        api_key=api_key,\n        base_url=base_url,\n    )\n    client = SyncLLMClient(llm_config)\n\n    agent_a = Agent(llm_client=client, prompt_registry=prompts_a)\n    agent_b = Agent(llm_client=client, prompt_registry=prompts_b)\n\n    message = \"Discuss the pros and cons of remote work.\"\n    for _ in range(3):\n        reply_a = agent_a.run(message)\n        print(\"\\nAgent A:\", reply_a)\n        reply_b = agent_b.run(reply_a)\n        print(\"\\nAgent B:\", reply_b)\n        message = reply_b\n\n    return 0\n\nif __name__ == \"__main__\":\n    raise SystemExit(main())\n</code></pre>"},{"location":"examples/#plannerexecutor-pattern","title":"Planner/Executor Pattern","text":"<p>One agent creates a plan, another executes it.</p> <p>File: <code>examples/05_multi_agent/planner_executor.py</code></p> <pre><code>#!/usr/bin/env python3\nimport os\nfrom iris_agent import Agent, SyncLLMClient, LLMConfig, LLMProvider, PromptRegistry\n\ndef main() -&gt; int:\n    api_key = os.getenv(\"OPENAI_API_KEY\")\n    model = os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\")\n    base_url = os.getenv(\"OPENAI_BASE_URL\")\n\n    if not api_key:\n        print(\"Set OPENAI_API_KEY before running this example.\")\n        return 1\n\n    prompts = PromptRegistry()\n    prompts.add_prompt(\n        \"planner_assistant\",\n        \"You are a planning agent. Produce a short numbered plan.\",\n    )\n\n    prompts.add_prompt(\n        \"executor_assistant\",\n        \"You are an execution agent. Follow the plan and answer succinctly.\",\n    )\n\n    llm_config = LLMConfig(\n        provider=LLMProvider.OPENAI,\n        model=model,\n        api_key=api_key,\n        base_url=base_url,\n    )\n    client = SyncLLMClient(llm_config)\n\n    planner = Agent(\n        llm_client=client,\n        prompt_registry=prompts,\n        system_prompt_name=\"planner_assistant\",\n    )\n    executor = Agent(\n        llm_client=client,\n        prompt_registry=prompts,\n        system_prompt_name=\"executor_assistant\",\n    )\n\n    task = \"Design a 1-day itinerary for Mumbai.\"\n    plan = planner.run(task)\n    print(\"\\nPlan:\\n\", plan)\n\n    response = executor.run(f\"Task: {task}\\nPlan:\\n{plan}\")\n    print(\"\\nExecution:\\n\", response)\n    return 0\n\nif __name__ == \"__main__\":\n    raise SystemExit(main())\n</code></pre>"},{"location":"examples/#advanced-features","title":"Advanced Features","text":""},{"location":"examples/#custom-llm-client","title":"Custom LLM Client","text":"<p>Create a mock LLM client for testing without API calls.</p> <p>File: <code>examples/06_custom_llm/mock_llm_client.py</code></p> <pre><code>#!/usr/bin/env python3\nimport asyncio\nfrom typing import Any, AsyncGenerator\nfrom iris_agent import Agent, PromptRegistry\n\nclass LocalEchoClient:\n    async def chat_completion(\n        self,\n        messages: list[dict],\n        tools: list[dict] | None = None,\n        temperature: float = 1.0,\n        json_response: bool = False,\n        max_completion_tokens: int | None = None,\n        seed: int | None = None,\n        reasoning_effort: str | None = None,\n        web_search_options: dict | None = None,\n    ) -&gt; dict:\n        last = messages[-1][\"content\"] if messages else \"\"\n        return {\n            \"content\": f\"Echo: {last}\",\n            \"tool_calls\": [],\n            \"message\": None,\n            \"finish_reason\": \"stop\",\n        }\n\n    async def chat_completion_stream(\n        self,\n        messages: list[dict],\n        tools: list[dict] | None = None,\n        temperature: float = 1.0,\n        json_response: bool = False,\n        max_tokens: int | None = None,\n        seed: int | None = None,\n        reasoning_effort: str | None = None,\n        web_search_options: dict | None = None,\n    ) -&gt; AsyncGenerator[Any, None]:\n        await asyncio.sleep(0)\n        if False:\n            yield None\n\ndef main() -&gt; int:\n    prompts = PromptRegistry()\n    prompts.add_prompt(\"assistant\", \"You are a mock assistant.\")\n\n    client = LocalEchoClient()\n    agent = Agent(llm_client=client, prompt_registry=prompts)\n\n    response = agent.run(\"Hello from the mock client.\")\n    print(response)\n    return 0\n\nif __name__ == \"__main__\":\n    raise SystemExit(main())\n</code></pre>"},{"location":"examples/#rich-logging","title":"Rich Logging","text":"<p>Enable beautiful, colorized terminal output showing agent steps.</p> <p>File: <code>examples/07_logging/rich_logging.py</code></p> <pre><code>#!/usr/bin/env python3\nimport os\nfrom iris_agent import Agent, SyncLLMClient, LLMConfig, LLMProvider, PromptRegistry\n\ndef main() -&gt; int:\n    api_key = os.getenv(\"OPENAI_API_KEY\")\n    model = os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\")\n    base_url = os.getenv(\"OPENAI_BASE_URL\")\n\n    if not api_key:\n        print(\"Set OPENAI_API_KEY before running this example.\")\n        return 1\n\n    prompts = PromptRegistry()\n    prompts.add_prompt(\"assistant\", \"You are a helpful assistant.\")\n\n    llm_config = LLMConfig(\n        provider=LLMProvider.OPENAI,\n        model=model,\n        api_key=api_key,\n        base_url=base_url,\n    )\n    client = SyncLLMClient(llm_config)\n\n    agent = Agent(\n        llm_client=client,\n        prompt_registry=prompts,\n        enable_logging=True,\n    )\n\n    response = agent.run(\"Summarize the benefits of good logging in one sentence.\")\n    print(response)\n    return 0\n\nif __name__ == \"__main__\":\n    raise SystemExit(main())\n</code></pre>"},{"location":"examples/#message-creation","title":"Message Creation","text":"<p>Create messages with text, images, and names.</p> <p>File: <code>examples/08_messages/create_message_examples.py</code></p> <pre><code>#!/usr/bin/env python3\nfrom iris_agent import Role, create_message\n\ndef main() -&gt; int:\n    text_msg = create_message(Role.USER, \"Hello\")\n    print(\"Text message:\", text_msg)\n\n    image_msg = create_message(\n        Role.USER,\n        \"Describe this image\",\n        images=[\"https://example.com/image.jpg\"],\n    )\n    print(\"Image message:\", image_msg)\n\n    named_msg = create_message(Role.USER, \"Hello\", name=\"John Doe\")\n    print(\"Named message:\", named_msg)\n    return 0\n\nif __name__ == \"__main__\":\n    raise SystemExit(main())\n</code></pre>"},{"location":"examples/#gemini-integration","title":"Gemini Integration","text":"<p>Use Google Gemini models via OpenAI-compatible endpoint.</p> <p>File: <code>examples/09_gemini/gemini_basic.py</code></p> <pre><code>#!/usr/bin/env python3\nimport os\nfrom iris_agent import Agent, SyncLLMClient, LLMConfig, LLMProvider, PromptRegistry\n\ndef main() -&gt; int:\n    api_key = os.getenv(\"GEMINI_API_KEY\")\n    base_url = os.getenv(\"GEMINI_BASE_URL\")\n    model = os.getenv(\"GEMINI_MODEL\", \"gemini-3-flash-preview\")\n\n    if not api_key:\n        print(\"Set GEMINI_API_KEY before running this example.\")\n        return 1\n\n    prompts = PromptRegistry()\n    prompts.add_prompt(\"assistant\", \"You are a helpful assistant.\")\n\n    llm_config = LLMConfig(\n        provider=LLMProvider.GOOGLE,\n        model=model,\n        api_key=api_key,\n        base_url=base_url,\n    )\n    client = SyncLLMClient(llm_config)\n\n    agent = Agent(llm_client=client, prompt_registry=prompts)\n    response = agent.run(\"Say hello in one sentence.\")\n    print(response)\n    return 0\n\nif __name__ == \"__main__\":\n    raise SystemExit(main())\n</code></pre>"},{"location":"examples/#additional-examples","title":"Additional Examples","text":""},{"location":"examples/#json-mode","title":"JSON Mode","text":"<p>Force the agent to output valid JSON only.</p> <pre><code>#!/usr/bin/env python3\nimport os\nfrom iris_agent import Agent, SyncLLMClient, LLMConfig, LLMProvider, PromptRegistry\n\ndef main() -&gt; int:\n    api_key = os.getenv(\"OPENAI_API_KEY\")\n    if not api_key:\n        print(\"Set OPENAI_API_KEY before running this example.\")\n        return 1\n\n    prompts = PromptRegistry()\n    prompts.add_prompt(\"assistant\", \"You are a helpful assistant that outputs JSON.\")\n\n    client = SyncLLMClient(LLMConfig(\n        provider=LLMProvider.OPENAI,\n        model=\"gpt-4o-mini\",\n        api_key=api_key\n    ))\n\n    agent = Agent(llm_client=client, prompt_registry=prompts)\n\n    response = agent.run(\n        \"Generate a list of 3 cities with their countries. Output JSON.\",\n        json_response=True\n    )\n    print(response)\n    return 0\n\nif __name__ == \"__main__\":\n    raise SystemExit(main())\n</code></pre>"},{"location":"examples/#multimodal-images","title":"Multimodal (Images)","text":"<p>Send images to vision-capable models.</p> <pre><code>#!/usr/bin/env python3\nimport os\nfrom iris_agent import Agent, SyncLLMClient, LLMConfig, LLMProvider, PromptRegistry, Role, create_message\n\ndef main() -&gt; int:\n    api_key = os.getenv(\"OPENAI_API_KEY\")\n    if not api_key:\n        print(\"Set OPENAI_API_KEY before running this example.\")\n        return 1\n\n    prompts = PromptRegistry()\n    prompts.add_prompt(\"assistant\", \"You are a helpful assistant that can analyze images.\")\n\n    client = SyncLLMClient(LLMConfig(\n        provider=LLMProvider.OPENAI,\n        model=\"gpt-4o\",  # Use vision-capable model\n        api_key=api_key\n    ))\n\n    agent = Agent(llm_client=client, prompt_registry=prompts)\n\n    # Create message with image\n    msg = create_message(\n        role=Role.USER,\n        content=\"What is in this image?\",\n        images=[\"https://example.com/photo.jpg\"]\n    )\n\n    response = agent.run(msg)\n    print(response)\n    return 0\n\nif __name__ == \"__main__\":\n    raise SystemExit(main())\n</code></pre>"},{"location":"examples/#tools-from-a-class","title":"Tools from a Class","text":"<p>Register all tools from a class at once.</p> <pre><code>#!/usr/bin/env python3\nimport os\nfrom iris_agent import ToolRegistry, tool, Agent, SyncLLMClient, LLMConfig, LLMProvider, PromptRegistry\n\nclass MathTools:\n    \"\"\"A class containing math-related tools.\"\"\"\n\n    @tool\n    def add(self, a: int, b: int) -&gt; int:\n        \"\"\"Add two numbers.\"\"\"\n        return a + b\n\n    @tool\n    def multiply(self, a: int, b: int) -&gt; int:\n        \"\"\"Multiply two numbers.\"\"\"\n        return a * b\n\n    @tool\n    def subtract(self, a: int, b: int) -&gt; int:\n        \"\"\"Subtract b from a.\"\"\"\n        return a - b\n\ndef main() -&gt; int:\n    api_key = os.getenv(\"OPENAI_API_KEY\")\n    if not api_key:\n        print(\"Set OPENAI_API_KEY before running this example.\")\n        return 1\n\n    # Create instance and register all tools\n    math_tools = MathTools()\n    tool_registry = ToolRegistry()\n    tool_registry.register_from(math_tools)\n\n    prompts = PromptRegistry()\n    prompts.add_prompt(\"assistant\", \"You are a helpful math assistant.\")\n\n    client = SyncLLMClient(LLMConfig(\n        provider=LLMProvider.OPENAI,\n        model=\"gpt-4o-mini\",\n        api_key=api_key\n    ))\n\n    agent = Agent(\n        llm_client=client,\n        prompt_registry=prompts,\n        tool_registry=tool_registry\n    )\n\n    response = agent.run(\"Calculate (10 + 5) * 3 - 7\")\n    print(response)\n    return 0\n\nif __name__ == \"__main__\":\n    raise SystemExit(main())\n</code></pre>"},{"location":"examples/#error-handling-in-tools","title":"Error Handling in Tools","text":"<p>Handle errors gracefully in tool functions.</p> <pre><code>#!/usr/bin/env python3\nimport os\nfrom iris_agent import ToolRegistry, tool, Agent, SyncLLMClient, LLMConfig, LLMProvider, PromptRegistry\n\n@tool\ndef divide(a: float, b: float) -&gt; float:\n    \"\"\"Divide a by b. Returns error message if b is zero.\"\"\"\n    if b == 0:\n        raise ValueError(\"Cannot divide by zero\")\n    return a / b\n\ndef main() -&gt; int:\n    api_key = os.getenv(\"OPENAI_API_KEY\")\n    if not api_key:\n        print(\"Set OPENAI_API_KEY before running this example.\")\n        return 1\n\n    tool_registry = ToolRegistry()\n    tool_registry.register(divide)\n\n    prompts = PromptRegistry()\n    prompts.add_prompt(\"assistant\", \"You are a helpful math assistant. If a tool returns an error, explain it to the user.\")\n\n    client = SyncLLMClient(LLMConfig(\n        provider=LLMProvider.OPENAI,\n        model=\"gpt-4o-mini\",\n        api_key=api_key\n    ))\n\n    agent = Agent(\n        llm_client=client,\n        prompt_registry=prompts,\n        tool_registry=tool_registry\n    )\n\n    # The agent will handle the error gracefully\n    response = agent.run(\"What is 10 divided by 0?\")\n    print(response)\n    return 0\n\nif __name__ == \"__main__\":\n    raise SystemExit(main())\n</code></pre>"},{"location":"examples/#running-examples","title":"Running Examples","text":"<p>All examples can be run directly from the repository root:</p> <pre><code># Basic examples\npython examples/01_basic/simple_agent.py\npython examples/01_basic/async_agent.py\npython examples/01_basic/streaming_agent.py\n\n# Tools\npython examples/03_tools/tool_agent_usage.py\n\n# Multi-agent\npython examples/05_multi_agent/planner_executor.py\n\n# And so on...\n</code></pre>"},{"location":"examples/#requirements","title":"Requirements","text":"<p>Most examples require: - <code>OPENAI_API_KEY</code> environment variable - Optional: <code>OPENAI_MODEL</code> (defaults are set in examples) - Optional: <code>OPENAI_BASE_URL</code></p> <p>Gemini examples require: - <code>GEMINI_API_KEY</code> environment variable - Optional: <code>GEMINI_MODEL</code> - Optional: <code>GEMINI_BASE_URL</code></p>"},{"location":"examples/#next-steps","title":"Next Steps","text":"<ul> <li>Read the Getting Started Guide for a step-by-step introduction</li> <li>Explore Core Concepts to understand the architecture</li> <li>Check How-To Guides for practical recipes</li> <li>Review the API Reference for detailed documentation</li> </ul>"},{"location":"faq/","title":"Frequently Asked Questions","text":""},{"location":"faq/#general","title":"General","text":""},{"location":"faq/#does-iris-agent-support-local-llms","title":"Does Iris Agent support local LLMs?","text":"<p>Yes. You can use any OpenAI-compatible API (Ollama, vLLM, LocalAI) by setting the <code>base_url</code> in <code>LLMConfig</code>.</p>"},{"location":"faq/#is-conversation-history-persisted-to-disk","title":"Is conversation history persisted to disk?","text":"<p>No. By default, memory is stored in a Python list (<code>agent.memory</code>). If you restart the script, memory is lost. You can easily implement persistence by saving/loading <code>agent.memory</code> to a JSON file or database.</p>"},{"location":"faq/#technical","title":"Technical","text":""},{"location":"faq/#how-do-i-clear-the-agents-memory","title":"How do I clear the agent's memory?","text":"<p>Simply assign a new list or call clear on the existing one, but remember to keep the system prompt if you want it to persist.</p> <pre><code># Keep system prompt (index 0)\nagent.memory = [agent.memory[0]]\n</code></pre>"},{"location":"faq/#can-i-share-tools-between-agents","title":"Can I share tools between agents?","text":"<p>Yes. You can pass the same <code>ToolRegistry</code> instance to multiple agents.</p>"},{"location":"faq/#is-it-thread-safe","title":"Is it thread-safe?","text":"<p>The <code>AsyncAgent</code> is designed for <code>asyncio</code> concurrency. The synchronous <code>Agent</code> is not thread-safe if shared across threads without locks, as it modifies internal state (memory). It is better to create one agent per session/user.</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>This guide will help you build your first AI agent using Iris Agent.</p>"},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10 or higher.</li> <li>An <code>api_key</code> if you want to use OpenAI or Google Gemini.</li> <li>An <code>api_key</code> with <code>base_url</code> if you want to use other model providers or local models which are compatible with OpenAI SDK.</li> </ul>"},{"location":"getting-started/#1-installation","title":"1. Installation","text":"<p>First, install the package:</p> <pre><code>pip install iris-agent\n</code></pre>"},{"location":"getting-started/#2-create-a-client","title":"2. Create a Client","text":"<p>To create an agent, you need an <code>LLMConfig</code> and a <code>SyncLLMClient</code>.</p> <pre><code>import os\nfrom iris_agent import LLMConfig, LLMProvider, SyncLLMClient\n\n# Ideally, load API keys from environment variables\napi_key = os.getenv(\"OPENAI_API_KEY\")\n\nconfig = LLMConfig(\n    provider=LLMProvider.OPENAI,\n    model=\"gpt-4o-mini\",\n    api_key=api_key\n)\n\nclient = SyncLLMClient(config)\n</code></pre>"},{"location":"getting-started/#3-add-a-system-prompt","title":"3. Add a System Prompt","text":"<p>Define system instructions using the <code>PromptRegistry</code> to customize your agent's behavior.</p> <pre><code>from iris_agent import PromptRegistry\n\n# Define system instructions\nprompt_registry = PromptRegistry()\nprompt_registry.add_prompt(\"assistant\", \"You are a friendly pirate assistant. Arr!\")\n</code></pre>"},{"location":"getting-started/#4-creating-your-first-agent","title":"4. Creating Your First Agent","text":"<p>Combine the client with an <code>Agent</code> instance and the prompt registry.</p> <pre><code>from iris_agent import Agent\n\nagent = Agent(\n    llm_client=client,\n    prompt_registry=prompt_registry,\n    system_prompt_name=\"assistant\"\n)\n\nresponse = agent.run(\"What is the capital of France?\")\nprint(response)\n# Output: Arr! The capital of France be Paris, matey!\n</code></pre>"},{"location":"getting-started/#5-adding-tools","title":"5. Adding Tools","text":"<p>Tools allow your agent to interact with the outside world. Use the <code>@tool</code> decorator.</p> <pre><code>from iris_agent import tool, ToolRegistry\n\ntool_registry = ToolRegistry()\n\n@tool\ndef get_weather(location: str) -&gt; str:\n    \"\"\"Get the current weather for a location.\"\"\"\n    # In a real app, call an API here\n    return f\"The weather in {location} is sunny and 25\u00b0C.\"\n\ntool_registry.register(get_weather)\n\n# Update the agent with the tool registry\nagent = Agent(\n    llm_client=client,\n    tool_registry=tool_registry,\n    prompt_registry=prompt_registry,\n    system_prompt_name=\"assistant\"\n)\n\nresponse = agent.run(\"What's the weather in Tokyo?\")\nprint(response)\n# Arr! The weather in Tokyo be sunny and a toasty 25\u00b0C, matey!\n</code></pre>"},{"location":"getting-started/#6-async-support","title":"6. Async Support","text":"<p>For high-performance applications (e.g., web servers), use <code>AsyncAgent</code>.</p> <pre><code>import asyncio\nfrom iris_agent import AsyncAgent, AsyncLLMClient\n\nasync def main():\n    api_key = os.getenv(\"OPENAI_API_KEY\")\n    model = os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\")\n    config = LLMConfig(\n        provider=LLMProvider.OPENAI,\n        model=model,\n        api_key=api_key\n    )\n    prompt_registry = PromptRegistry()\n    prompt_registry.add_prompt(\"assistant\", \"You are a helpful assistant.\")\n    client = AsyncLLMClient(config)\n    agent = AsyncAgent(\n        llm_client=client, \n        prompt_registry=prompt_registry, \n        system_prompt_name=\"assistant\"\n        )\n    response = await agent.run(\"Tell me a quick joke.\")\n    print(response)\n    # Hey, what do you call a function that only tells the truth? A literal... function.\n\n\n\nasyncio.run(main())\n</code></pre>"},{"location":"getting-started/#7-streaming","title":"7. Streaming","text":"<p>Stream responses to get tokens as they're generated. Both sync and async agents support streaming.</p> <p>Sync Streaming:</p> <pre><code>from iris_agent import Agent\n\nagent = Agent(llm_client=client)\nfor chunk in agent.run_stream(\"Tell me a short story.\"):\n    print(chunk, end=\"\", flush=True)\nprint()\n</code></pre> <p>Async Streaming:</p> <pre><code>import asyncio\nfrom iris_agent import AsyncAgent\n\nasync def main():\n    agent = AsyncAgent(llm_client=client)\n    async for chunk in agent.run_stream(\"Tell me a short story.\"):\n        print(chunk, end=\"\", flush=True)\n    print()\n\nasyncio.run(main())\n</code></pre>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Core Concepts like Memory and Tools.</li> <li>See more Examples including multi-agent setups.</li> <li>Check the API Reference for detailed documentation.</li> </ul>"},{"location":"how-to/","title":"How-To Guides","text":"<p>This section provides practical guides for common tasks.</p>"},{"location":"how-to/#how-to-define-tools","title":"How to Define Tools","text":"<p>Tools are the primary way your agent interacts with the world.</p>"},{"location":"how-to/#1-simple-function-tool","title":"1. Simple Function Tool","text":"<pre><code>import os\nfrom iris_agent import tool, ToolRegistry, Agent, SyncLLMClient, LLMConfig, LLMProvider, PromptRegistry\n\n@tool\ndef add(a: int, b: int) -&gt; int:\n    \"\"\"Add two numbers.\"\"\"\n    return a + b\n\n# Register the tool\ntool_registry = ToolRegistry()\ntool_registry.register(add)\n\n# Create client and agent\nclient = SyncLLMClient(LLMConfig(\n    provider=LLMProvider.OPENAI,\n    model=\"gpt-4o-mini\",\n    api_key=os.getenv(\"OPENAI_API_KEY\")\n))\nprompt_registry = PromptRegistry()\nprompt_registry.add_prompt(\"assistant\", \"You are a helpful math assistant.\")\n\nagent = Agent(\n    llm_client=client,\n    prompt_registry=prompt_registry,\n    tool_registry=tool_registry\n)\n\n# Use the agent with tools\nresponse = agent.run(\"What is 12 + 30? Use the add tool.\")\nprint(response)\n</code></pre>"},{"location":"how-to/#2-tool-with-complex-types","title":"2. Tool with Complex Types","text":"<p>Iris Agent supports <code>List</code>, <code>Dict</code>, and <code>Literal</code> for validation.</p> <pre><code>import os\nfrom typing import Literal, List\nfrom iris_agent import tool, ToolRegistry, Agent, SyncLLMClient, LLMConfig, LLMProvider, PromptRegistry\n\n@tool\ndef search_products(\n    query: str,\n    category: Literal[\"electronics\", \"clothing\", \"books\"],\n    tags: List[str] = None\n) -&gt; str:\n    \"\"\"Search for products in a specific category.\"\"\"\n    # Implementation...\n    return f\"Found 5 items in {category} matching '{query}'\"\n\n# Register the tool\ntool_registry = ToolRegistry()\ntool_registry.register(search_products)\n\n# Create client and agent\nclient = SyncLLMClient(LLMConfig(\n    provider=LLMProvider.OPENAI,\n    model=\"gpt-4o-mini\",\n    api_key=os.getenv(\"OPENAI_API_KEY\")\n))\nprompt_registry = PromptRegistry()\nprompt_registry.add_prompt(\"assistant\", \"You are a helpful shopping assistant.\")\n\nagent = Agent(\n    llm_client=client,\n    prompt_registry=prompt_registry,\n    tool_registry=tool_registry\n)\n\n# Use the agent with tools\nresponse = agent.run(\"Search for laptops in electronics category.\")\nprint(response)\n</code></pre>"},{"location":"how-to/#3-registering-multiple-tools","title":"3. Registering Multiple Tools","text":"<pre><code>import os\nfrom iris_agent import ToolRegistry, Agent, SyncLLMClient, LLMConfig, LLMProvider, PromptRegistry, tool\n\n@tool\ndef add(a: int, b: int) -&gt; int:\n    \"\"\"Add two numbers.\"\"\"\n    return a + b\n\n@tool\ndef multiply(a: int, b: int) -&gt; int:\n    \"\"\"Multiply two numbers.\"\"\"\n    return a * b\n\n# Register all tools\ntool_registry = ToolRegistry()\ntool_registry.register(add)\ntool_registry.register(multiply)\n\n# Create client and agent\nclient = SyncLLMClient(LLMConfig(\n    provider=LLMProvider.OPENAI,\n    model=\"gpt-4o-mini\",\n    api_key=os.getenv(\"OPENAI_API_KEY\")\n))\nprompt_registry = PromptRegistry()\nprompt_registry.add_prompt(\"assistant\", \"You are a helpful math assistant.\")\n\nagent = Agent(\n    llm_client=client,\n    prompt_registry=prompt_registry,\n    tool_registry=tool_registry\n)\n\n# Use the agent with multiple tools\nresponse = agent.run(\"Calculate 5 * 7 and then add 10 to the result.\")\nprint(response)\n</code></pre>"},{"location":"how-to/#4-registering-tools-from-a-class","title":"4. Registering Tools from a Class","text":"<p>You can organize tools in a class and register all of them at once using <code>register_from()</code>:</p> <pre><code>import os\nfrom iris_agent import ToolRegistry, Agent, SyncLLMClient, LLMConfig, LLMProvider, PromptRegistry, tool\n\nclass MathTools:\n    \"\"\"A class containing math-related tools.\"\"\"\n\n    @tool\n    def add(self, a: int, b: int) -&gt; int:\n        \"\"\"Add two numbers.\"\"\"\n        return a + b\n\n    @tool\n    def multiply(self, a: int, b: int) -&gt; int:\n        \"\"\"Multiply two numbers.\"\"\"\n        return a * b\n\n    @tool\n    def subtract(self, a: int, b: int) -&gt; int:\n        \"\"\"Subtract b from a.\"\"\"\n        return a - b\n\n    @tool\n    def divide(self, a: float, b: float) -&gt; float:\n        \"\"\"Divide a by b.\"\"\"\n        if b == 0:\n            raise ValueError(\"Cannot divide by zero\")\n        return a / b\n\n# Create an instance of the class\nmath_tools = MathTools()\n\n# Register all tools from the class at once\ntool_registry = ToolRegistry()\ntool_registry.register_from(math_tools)\n\n# Create client and agent\nclient = SyncLLMClient(LLMConfig(\n    provider=LLMProvider.OPENAI,\n    model=\"gpt-4o-mini\",\n    api_key=os.getenv(\"OPENAI_API_KEY\")\n))\nprompt_registry = PromptRegistry()\nprompt_registry.add_prompt(\"assistant\", \"You are a helpful math assistant with access to various math operations.\")\n\nagent = Agent(\n    llm_client=client,\n    prompt_registry=prompt_registry,\n    tool_registry=tool_registry\n)\n\n# Use the agent with all the class tools\nresponse = agent.run(\"Calculate (10 + 5) * 3 - 7, then divide the result by 2.\")\nprint(response)\n</code></pre> <p>Note: <code>register_from()</code> automatically scans the object for all methods decorated with <code>@tool</code> and registers them. This is useful for organizing related tools together.</p>"},{"location":"how-to/#how-to-stream-responses","title":"How to Stream Responses","text":"<p>Streaming is essential for a responsive UI. You can stream with <code>AsyncAgent</code> or use <code>Agent.run_stream()</code> for a sync-friendly iterator.</p>"},{"location":"how-to/#async-streaming","title":"Async Streaming","text":"<pre><code>import asyncio\nimport os\nfrom iris_agent import AsyncAgent, AsyncLLMClient, LLMConfig, LLMProvider, PromptRegistry\n\nasync def stream_chat():\n    # Create client\n    client = AsyncLLMClient(LLMConfig(\n        provider=LLMProvider.OPENAI,\n        model=\"gpt-4o-mini\",\n        api_key=os.getenv(\"OPENAI_API_KEY\")\n    ))\n\n    # Create prompt registry\n    prompt_registry = PromptRegistry()\n    prompt_registry.add_prompt(\"assistant\", \"You are a helpful assistant.\")\n\n    # Create agent\n    agent = AsyncAgent(\n        llm_client=client,\n        prompt_registry=prompt_registry\n    )\n\n    # run_stream yields chunks of text\n    async for chunk in agent.run_stream(\"Tell me a short story about a robot.\"):\n        print(chunk, end=\"\", flush=True)\n    print()  # New line after streaming\n\nasyncio.run(stream_chat())\n</code></pre>"},{"location":"how-to/#sync-streaming","title":"Sync Streaming","text":"<pre><code>import os\nfrom iris_agent import Agent, SyncLLMClient, LLMConfig, LLMProvider, PromptRegistry\n\n# Create client\nclient = SyncLLMClient(LLMConfig(\n    provider=LLMProvider.OPENAI,\n    model=\"gpt-4o-mini\",\n    api_key=os.getenv(\"OPENAI_API_KEY\")\n))\n\n# Create prompt registry\nprompt_registry = PromptRegistry()\nprompt_registry.add_prompt(\"assistant\", \"You are a helpful assistant.\")\n\n# Create agent\nagent = Agent(\n    llm_client=client,\n    prompt_registry=prompt_registry\n)\n\n# Stream responses\nfor chunk in agent.run_stream(\"Tell me a short story about a robot.\"):\n    print(chunk, end=\"\", flush=True)\nprint()  # New line after streaming\n</code></pre>"},{"location":"how-to/#how-to-use-json-mode","title":"How to Use JSON Mode","text":"<p>If you need the agent to output strict JSON, use the <code>json_response</code> parameter.</p>"},{"location":"how-to/#async-json-mode","title":"Async JSON Mode","text":"<pre><code>import asyncio\nimport os\nfrom iris_agent import AsyncAgent, AsyncLLMClient, LLMConfig, LLMProvider, PromptRegistry\n\nasync def get_json_data():\n    # Create client\n    client = AsyncLLMClient(LLMConfig(\n        provider=LLMProvider.OPENAI,\n        model=\"gpt-4o-mini\",\n        api_key=os.getenv(\"OPENAI_API_KEY\")\n    ))\n\n    # Create prompt registry\n    prompt_registry = PromptRegistry()\n    prompt_registry.add_prompt(\"assistant\", \"You are a helpful assistant that outputs JSON.\")\n\n    # Create agent\n    agent = AsyncAgent(\n        llm_client=client,\n        prompt_registry=prompt_registry\n    )\n\n    prompt = \"Generate a list of 3 cities with lat/long.\"\n\n    # Ensure you mention JSON in the prompt as well for best results\n    response = await agent.run(\n        prompt + \" Output JSON.\",\n        json_response=True\n    )\n    print(response)\n    # Output: {\"cities\": [...]}\n\nasyncio.run(get_json_data())\n</code></pre>"},{"location":"how-to/#sync-json-mode","title":"Sync JSON Mode","text":"<pre><code>import os\nfrom iris_agent import Agent, SyncLLMClient, LLMConfig, LLMProvider, PromptRegistry\n\n# Create client\nclient = SyncLLMClient(LLMConfig(\n    provider=LLMProvider.OPENAI,\n    model=\"gpt-4o-mini\",\n    api_key=os.getenv(\"OPENAI_API_KEY\")\n))\n\n# Create prompt registry\nprompt_registry = PromptRegistry()\nprompt_registry.add_prompt(\"assistant\", \"You are a helpful assistant that outputs JSON.\")\n\n# Create agent\nagent = Agent(\n    llm_client=client,\n    prompt_registry=prompt_registry\n)\n\n# Use JSON mode\nresponse = agent.run(\n    \"Generate a list of 3 cities with lat/long. Output JSON.\",\n    json_response=True\n)\nprint(response)\n</code></pre>"},{"location":"how-to/#how-to-handle-images-multimodal","title":"How to Handle Images (Multimodal)","text":"<p>Use <code>create_message</code> to send images to models like GPT-4o or Gemini 1.5 Pro.</p> <pre><code>import os\nfrom iris_agent import create_message, Role, Agent, SyncLLMClient, LLMConfig, LLMProvider, PromptRegistry\n\n# Create client (GPT-4o or Gemini 1.5 Pro support images)\nclient = SyncLLMClient(LLMConfig(\n    provider=LLMProvider.OPENAI,\n    model=\"gpt-4o\",  # Use gpt-4o for image support\n    api_key=os.getenv(\"OPENAI_API_KEY\")\n))\n\n# Create prompt registry\nprompt_registry = PromptRegistry()\nprompt_registry.add_prompt(\"assistant\", \"You are a helpful assistant that can analyze images.\")\n\n# Create agent\nagent = Agent(\n    llm_client=client,\n    prompt_registry=prompt_registry\n)\n\n# Create a message with an image\nmsg = create_message(\n    role=Role.USER,\n    content=\"What is in this image?\",\n    images=[\"https://example.com/photo.jpg\"]\n)\n\n# Send the message to the agent\nresponse = agent.run(msg)\nprint(response)\n</code></pre>"},{"location":"how-to/#multiple-images","title":"Multiple Images","text":"<pre><code># You can also send multiple images\nmsg = create_message(\n    role=Role.USER,\n    content=\"Compare these two images.\",\n    images=[\n        \"https://example.com/image1.jpg\",\n        \"https://example.com/image2.jpg\"\n    ]\n)\n\nresponse = agent.run(msg)\nprint(response)\n</code></pre>"},{"location":"how-to/#image-only-messages","title":"Image-Only Messages","text":"<pre><code># For image-only messages (no text), use empty content\nmsg = create_message(\n    role=Role.USER,\n    content=\"\",  # Empty string for image-only\n    images=[\"https://example.com/chart.png\"]\n)\n\nresponse = agent.run(msg)\nprint(response)\n</code></pre>"},{"location":"how-to/#how-to-use-gemini-openai-compatibility","title":"How to Use Gemini OpenAI Compatibility","text":"<p>Gemini models are accessible through the OpenAI-compatible endpoint. When you set <code>provider=LLMProvider.GOOGLE</code>, Iris Agent defaults to the Gemini OpenAI base URL and reads <code>GEMINI_API_KEY</code> if <code>api_key</code> is not provided.</p>"},{"location":"how-to/#basic-gemini-usage","title":"Basic Gemini Usage","text":"<pre><code>import os\nfrom iris_agent import Agent, LLMConfig, LLMProvider, SyncLLMClient, PromptRegistry\n\n# Create Gemini client\nconfig = LLMConfig(\n    provider=LLMProvider.GOOGLE,\n    model=\"gemini-3-flash-preview\",\n    api_key=os.getenv(\"GEMINI_API_KEY\"),\n)\nclient = SyncLLMClient(config)\n\n# Create prompt registry\nprompt_registry = PromptRegistry()\nprompt_registry.add_prompt(\"assistant\", \"You are a helpful assistant.\")\n\n# Create agent\nagent = Agent(\n    llm_client=client,\n    prompt_registry=prompt_registry\n)\n\nresponse = agent.run(\"Explain how AI works.\")\nprint(response)\n</code></pre>"},{"location":"how-to/#gemini-with-thinking-config","title":"Gemini with Thinking Config","text":"<p>To send Gemini-specific fields (like <code>thinking_config</code>), pass <code>extra_body</code>:</p> <pre><code>import os\nfrom iris_agent import Agent, LLMConfig, LLMProvider, SyncLLMClient, PromptRegistry\n\n# Create Gemini client\nconfig = LLMConfig(\n    provider=LLMProvider.GOOGLE,\n    model=\"gemini-2.0-flash-exp\",\n    api_key=os.getenv(\"GEMINI_API_KEY\"),\n)\nclient = SyncLLMClient(config)\n\n# Create prompt registry\nprompt_registry = PromptRegistry()\nprompt_registry.add_prompt(\"assistant\", \"You are a helpful assistant.\")\n\n# Create agent\nagent = Agent(\n    llm_client=client,\n    prompt_registry=prompt_registry\n)\n\n# Use with thinking config\nresponse = agent.run(\n    \"Explain how AI works.\",\n    extra_body={\n        \"google\": {\n            \"thinking_config\": {\n                \"thinking_budget\": \"low\",\n                \"include_thoughts\": True,\n            }\n        }\n    },\n)\nprint(response)\n</code></pre>"},{"location":"how-to/#how-to-use-prompt-registry","title":"How to Use Prompt Registry","text":"<p>The <code>PromptRegistry</code> is a powerful system for managing system instructions, personas, and dynamic prompt generation. It supports three types of prompts: static strings, template strings with placeholders, and callable functions.</p>"},{"location":"how-to/#why-use-promptregistry","title":"Why Use PromptRegistry?","text":"<p>Instead of hardcoding system prompts in your agent initialization, the <code>PromptRegistry</code> provides:</p> <ul> <li>Centralized Management: All prompts in one place, easy to update and version.</li> <li>Dynamic Generation: Create prompts on-the-fly based on context, user data, or configuration.</li> <li>Reusability: Share the same registry across multiple agents with different personas.</li> <li>Template Support: Use Python's <code>.format()</code> syntax for simple variable substitution.</li> </ul>"},{"location":"how-to/#1-static-prompts","title":"1. Static Prompts","text":"<p>The simplest form is a static string prompt:</p> <pre><code>import os\nfrom iris_agent import PromptRegistry, Agent, SyncLLMClient, LLMConfig, LLMProvider\n\nregistry = PromptRegistry()\nregistry.add_prompt(\"assistant\", \"You are a helpful AI assistant.\")\n\n# Create client and agent\nclient = SyncLLMClient(LLMConfig(\n    provider=LLMProvider.OPENAI,\n    model=\"gpt-4o-mini\",\n    api_key=os.getenv(\"OPENAI_API_KEY\")\n))\nagent = Agent(\n    llm_client=client,\n    prompt_registry=registry,\n    system_prompt_name=\"assistant\"\n)\n\n# Use the agent\nresponse = agent.run(\"Hello, how are you?\")\nprint(response)\n</code></pre>"},{"location":"how-to/#2-template-prompts-string-formatting","title":"2. Template Prompts (String Formatting)","text":"<p>You can use Python's <code>.format()</code> syntax to create dynamic prompts. Since agents render prompts without arguments, you need to pre-render template prompts:</p> <p>Option A: Pre-render and add to registry</p> <pre><code>import os\nfrom iris_agent import PromptRegistry, Agent, SyncLLMClient, LLMConfig, LLMProvider\n\nregistry = PromptRegistry()\nregistry.add_prompt(\"personal_assistant\", \"You are {name}'s personal assistant. Today is {date}.\")\n\n# Render with variables\nprompt_text = registry.render(\"personal_assistant\", name=\"Alice\", date=\"2024-01-15\")\n# Result: \"You are Alice's personal assistant. Today is 2024-01-15.\"\n\n# Add the rendered prompt back to the registry\nregistry.add_prompt(\"alice_assistant\", prompt_text)\n\n# Create client and agent\nclient = SyncLLMClient(LLMConfig(\n    provider=LLMProvider.OPENAI,\n    model=\"gpt-4o-mini\",\n    api_key=os.getenv(\"OPENAI_API_KEY\")\n))\nagent = Agent(\n    llm_client=client,\n    prompt_registry=registry,\n    system_prompt_name=\"alice_assistant\"\n)\n\n# Use the agent\nresponse = agent.run(\"What can you help me with today?\")\nprint(response)\n</code></pre> <p>Option B: Use callable prompts (recommended)</p> <pre><code>import os\nfrom iris_agent import PromptRegistry, Agent, SyncLLMClient, LLMConfig, LLMProvider\n\ndef personal_assistant_prompt(name: str = \"User\", date: str = \"\") -&gt; str:\n    \"\"\"Generate personalized assistant prompt.\"\"\"\n    date_str = f\" Today is {date}.\" if date else \"\"\n    return f\"You are {name}'s personal assistant.{date_str}\"\n\nregistry = PromptRegistry()\nregistry.add_prompt(\"personal_assistant\", personal_assistant_prompt)\n\n# Create client\nclient = SyncLLMClient(LLMConfig(\n    provider=LLMProvider.OPENAI,\n    model=\"gpt-4o-mini\",\n    api_key=os.getenv(\"OPENAI_API_KEY\")\n))\n\n# Option 1: Use with defaults (if function has defaults)\nagent = Agent(\n    llm_client=client,\n    prompt_registry=registry,\n    system_prompt_name=\"personal_assistant\"\n)\n\n# Option 2: Render with specific values and add to registry\nrendered = registry.render(\"personal_assistant\", name=\"Alice\", date=\"2024-01-15\")\nregistry.add_prompt(\"alice_assistant\", rendered)\nagent = Agent(\n    llm_client=client,\n    prompt_registry=registry,\n    system_prompt_name=\"alice_assistant\"\n)\n\n# Use the agent\nresponse = agent.run(\"What can you help me with?\")\nprint(response)\n</code></pre>"},{"location":"how-to/#3-callable-prompts-function-based","title":"3. Callable Prompts (Function-Based)","text":"<p>For maximum flexibility, you can register a function that generates the prompt dynamically:</p> <pre><code>import os\nfrom iris_agent import PromptRegistry, Agent, SyncLLMClient, LLMConfig, LLMProvider\n\ndef create_assistant_prompt(user_name: str, context: str = \"\") -&gt; str:\n    \"\"\"Generate a personalized assistant prompt.\"\"\"\n    base = f\"You are {user_name}'s assistant.\"\n    if context:\n        base += f\" Context: {context}\"\n    return base\n\nregistry = PromptRegistry()\nregistry.add_prompt(\"assistant\", create_assistant_prompt)\n\n# Render with arguments\nprompt = registry.render(\"assistant\", user_name=\"Bob\", context=\"coding session\")\n# Result: \"You are Bob's assistant. Context: coding session\"\n\n# Add the rendered prompt to registry\nregistry.add_prompt(\"bob_assistant\", prompt)\n\n# Create client and agent\nclient = SyncLLMClient(LLMConfig(\n    provider=LLMProvider.OPENAI,\n    model=\"gpt-4o-mini\",\n    api_key=os.getenv(\"OPENAI_API_KEY\")\n))\nagent = Agent(\n    llm_client=client,\n    prompt_registry=registry,\n    system_prompt_name=\"bob_assistant\"\n)\n\n# Use the agent\nresponse = agent.run(\"What can you help me with?\")\nprint(response)\n</code></pre>"},{"location":"how-to/#4-using-multiple-prompts-with-different-agents","title":"4. Using Multiple Prompts with Different Agents","text":"<p>The <code>PromptRegistry</code> integrates seamlessly with agents, allowing you to create multiple agents with different personas:</p> <pre><code>import os\nfrom iris_agent import PromptRegistry, Agent, SyncLLMClient, LLMConfig, LLMProvider\n\nregistry = PromptRegistry()\nregistry.add_prompt(\"coder\", \"You are an expert Python developer.\")\nregistry.add_prompt(\"writer\", \"You are a creative writing assistant.\")\n\n# Create client\nclient = SyncLLMClient(LLMConfig(\n    provider=LLMProvider.OPENAI,\n    model=\"gpt-4o-mini\",\n    api_key=os.getenv(\"OPENAI_API_KEY\")\n))\n\n# Agent 1: Uses \"coder\" prompt\ndev_agent = Agent(\n    llm_client=client,\n    prompt_registry=registry,\n    system_prompt_name=\"coder\"\n)\n\n# Agent 2: Uses \"writer\" prompt\nwriter_agent = Agent(\n    llm_client=client,\n    prompt_registry=registry,\n    system_prompt_name=\"writer\"\n)\n\n# Use the agents\ncode_response = dev_agent.run(\"Write a Python function to calculate factorial.\")\nprint(\"Coder:\", code_response)\n\nstory_response = writer_agent.run(\"Write a short story about a robot.\")\nprint(\"Writer:\", story_response)\n</code></pre> <p>The agent automatically: 1. Looks up the prompt by <code>system_prompt_name</code> 2. Renders it (if it's a template or callable, you may need to provide kwargs) 3. Inserts it as the first message in memory (role: \"developer\")</p>"},{"location":"how-to/#5-advanced-use-cases","title":"5. Advanced Use Cases","text":""},{"location":"how-to/#multi-tenant-applications","title":"Multi-Tenant Applications","text":"<pre><code>import os\nfrom iris_agent import PromptRegistry, Agent, SyncLLMClient, LLMConfig, LLMProvider\n\ndef tenant_prompt(tenant_id: str, tenant_config: dict) -&gt; str:\n    \"\"\"Generate prompt based on tenant configuration.\"\"\"\n    style = tenant_config.get(\"style\", \"professional\")\n    domain = tenant_config.get(\"domain\", \"general\")\n    return f\"You are a {style} assistant specializing in {domain} for tenant {tenant_id}.\"\n\nregistry = PromptRegistry()\nregistry.add_prompt(\"tenant\", tenant_prompt)\n\n# Create client\nclient = SyncLLMClient(LLMConfig(\n    provider=LLMProvider.OPENAI,\n    model=\"gpt-4o-mini\",\n    api_key=os.getenv(\"OPENAI_API_KEY\")\n))\n\n# Use in a web application\ndef get_tenant_config(tenant_id: str):\n    # Mock function - replace with actual config retrieval\n    return {\"style\": \"professional\", \"domain\": \"e-commerce\"}\n\ndef get_agent_for_tenant(tenant_id: str):\n    config = get_tenant_config(tenant_id)\n    # Render prompt with tenant-specific config\n    rendered = registry.render(\"tenant\", tenant_id=tenant_id, tenant_config=config)\n    registry.add_prompt(f\"tenant_{tenant_id}\", rendered)\n\n    return Agent(\n        llm_client=client,\n        prompt_registry=registry,\n        system_prompt_name=f\"tenant_{tenant_id}\"\n    )\n\n# Use it\nagent = get_agent_for_tenant(\"tenant_123\")\nresponse = agent.run(\"How can you help me?\")\nprint(response)\n</code></pre>"},{"location":"how-to/#ab-testing-prompts","title":"A/B Testing Prompts","text":"<pre><code>import os\nfrom iris_agent import PromptRegistry, Agent, SyncLLMClient, LLMConfig, LLMProvider\n\nregistry = PromptRegistry()\nregistry.add_prompt(\"variant_a\", \"You are a concise assistant.\")\nregistry.add_prompt(\"variant_b\", \"You are a detailed, thorough assistant.\")\n\n# Create client\nclient = SyncLLMClient(LLMConfig(\n    provider=LLMProvider.OPENAI,\n    model=\"gpt-4o-mini\",\n    api_key=os.getenv(\"OPENAI_API_KEY\")\n))\n\n# Switch between variants based on user\nuser_id = 123  # Example user ID\nvariant = \"variant_a\" if user_id % 2 == 0 else \"variant_b\"\nagent = Agent(\n    llm_client=client,\n    prompt_registry=registry,\n    system_prompt_name=variant\n)\n\n# Use the agent\nresponse = agent.run(\"Explain quantum computing.\")\nprint(response)\n</code></pre>"},{"location":"how-to/#context-aware-prompts","title":"Context-Aware Prompts","text":"<pre><code>import os\nfrom datetime import datetime\nfrom iris_agent import PromptRegistry, Agent, SyncLLMClient, LLMConfig, LLMProvider\n\ndef contextual_prompt(current_time: str, user_location: str) -&gt; str:\n    return f\"\"\"You are a helpful assistant.\nCurrent time: {current_time}\nUser location: {user_location}\nAdjust your responses based on timezone and location.\"\"\"\n\nregistry = PromptRegistry()\nregistry.add_prompt(\"contextual\", contextual_prompt)\n\n# Render with current context\nprompt = registry.render(\n    \"contextual\",\n    current_time=datetime.now().isoformat(),\n    user_location=\"New York\"\n)\n\n# Add rendered prompt to registry\nregistry.add_prompt(\"contextual_ny\", prompt)\n\n# Create client and agent\nclient = SyncLLMClient(LLMConfig(\n    provider=LLMProvider.OPENAI,\n    model=\"gpt-4o-mini\",\n    api_key=os.getenv(\"OPENAI_API_KEY\")\n))\nagent = Agent(\n    llm_client=client,\n    prompt_registry=registry,\n    system_prompt_name=\"contextual_ny\"\n)\n\n# Use the agent\nresponse = agent.run(\"What's the weather like?\")\nprint(response)\n</code></pre>"},{"location":"how-to/#best-practices","title":"Best Practices","text":"<ol> <li>Name Your Prompts Clearly: Use descriptive names like <code>\"customer_support\"</code> instead of <code>\"prompt1\"</code>.</li> <li>Keep Prompts Focused: Each prompt should define a single persona or role.</li> <li>Use Callables for Complex Logic: If you need database lookups, API calls, or complex string manipulation, use callable prompts.</li> <li>Document Your Prompts: Consider maintaining a separate file or docstring explaining what each prompt does.</li> <li>Version Your Prompts: For production systems, consider adding version numbers to prompt names (e.g., <code>\"assistant_v2\"</code>).</li> </ol>"},{"location":"how-to/#prompt-rendering-flow","title":"Prompt Rendering Flow","text":"<p>When an agent is initialized or when you manually render:</p> <pre><code># 1. Lookup\nprompt_template = registry.get_prompt(\"assistant\")\n\n# 2. Render (if needed)\nif callable(prompt_template):\n    rendered = prompt_template(**kwargs)  # Call the function\nelif isinstance(prompt_template, str) and kwargs:\n    rendered = prompt_template.format(**kwargs)  # Format string\nelse:\n    rendered = prompt_template  # Use as-is\n\n# 3. Use in agent memory\nagent.memory[0] = {\"role\": \"developer\", \"content\": rendered}\n</code></pre>"},{"location":"how-to/#how-to-manage-memory","title":"How to Manage Memory","text":"<p>Iris Agent automatically manages conversation history in the <code>memory</code> attribute. You can inspect, seed, and clear memory as needed.</p>"},{"location":"how-to/#inspecting-memory","title":"Inspecting Memory","text":"<pre><code>import os\nfrom iris_agent import Agent, SyncLLMClient, LLMConfig, LLMProvider, PromptRegistry\n\n# Create client and agent\nclient = SyncLLMClient(LLMConfig(\n    provider=LLMProvider.OPENAI,\n    model=\"gpt-4o-mini\",\n    api_key=os.getenv(\"OPENAI_API_KEY\")\n))\nprompt_registry = PromptRegistry()\nprompt_registry.add_prompt(\"assistant\", \"You are a helpful assistant.\")\n\nagent = Agent(\n    llm_client=client,\n    prompt_registry=prompt_registry\n)\n\n# Check initial memory (contains system prompt)\nprint(\"Initial memory:\", agent.memory)\n\n# Run a conversation\nresponse = agent.run(\"Hello!\")\nprint(\"Response:\", response)\n\n# Check memory after conversation\nprint(\"Memory after conversation:\", agent.memory)\nprint(\"Memory size:\", len(agent.memory))\n</code></pre>"},{"location":"how-to/#seeding-memory","title":"Seeding Memory","text":"<p>You can add messages to memory before starting a conversation:</p> <pre><code>import os\nfrom iris_agent import Agent, SyncLLMClient, LLMConfig, LLMProvider, PromptRegistry, Role, create_message\n\n# Create client and agent\nclient = SyncLLMClient(LLMConfig(\n    provider=LLMProvider.OPENAI,\n    model=\"gpt-4o-mini\",\n    api_key=os.getenv(\"OPENAI_API_KEY\")\n))\nprompt_registry = PromptRegistry()\nprompt_registry.add_prompt(\"assistant\", \"You are a helpful assistant.\")\n\nagent = Agent(\n    llm_client=client,\n    prompt_registry=prompt_registry\n)\n\n# Seed memory with previous context\nagent.memory.append(create_message(Role.USER, \"My name is Alice and I love Python.\"))\nagent.memory.append(create_message(Role.ASSISTANT, \"Nice to meet you, Alice!\"))\n\n# Now continue the conversation\nresponse = agent.run(\"What's my favorite programming language?\")\nprint(response)  # The agent will remember Alice loves Python\n</code></pre>"},{"location":"how-to/#clearing-memory","title":"Clearing Memory","text":"<p>You can clear the conversation history while keeping the system prompt:</p> <pre><code>import os\nfrom iris_agent import Agent, SyncLLMClient, LLMConfig, LLMProvider, PromptRegistry\n\n# Create client and agent\nclient = SyncLLMClient(LLMConfig(\n    provider=LLMProvider.OPENAI,\n    model=\"gpt-4o-mini\",\n    api_key=os.getenv(\"OPENAI_API_KEY\")\n))\nprompt_registry = PromptRegistry()\nprompt_registry.add_prompt(\"assistant\", \"You are a helpful assistant.\")\n\nagent = Agent(\n    llm_client=client,\n    prompt_registry=prompt_registry\n)\n\n# Have a conversation\nagent.run(\"Hello!\")\nagent.run(\"How are you?\")\n\nprint(\"Memory before clear:\", len(agent.memory))\n\n# Clear conversation history (system prompt remains)\nagent.memory.clear()\n\n# Re-add system prompt if needed\nagent._ensure_system_prompt()\n\nprint(\"Memory after clear:\", len(agent.memory))\n</code></pre>"},{"location":"how-to/#how-to-enable-logging","title":"How to Enable Logging","text":"<p>Iris Agent includes built-in Rich logging for beautiful, colorized terminal output showing the agent's step-by-step process.</p>"},{"location":"how-to/#basic-logging","title":"Basic Logging","text":"<pre><code>import os\nfrom iris_agent import Agent, SyncLLMClient, LLMConfig, LLMProvider, PromptRegistry\n\n# Create client\nclient = SyncLLMClient(LLMConfig(\n    provider=LLMProvider.OPENAI,\n    model=\"gpt-4o-mini\",\n    api_key=os.getenv(\"OPENAI_API_KEY\")\n))\n\n# Create prompt registry\nprompt_registry = PromptRegistry()\nprompt_registry.add_prompt(\"assistant\", \"You are a helpful assistant.\")\n\n# Enable logging by setting enable_logging=True\nagent = Agent(\n    llm_client=client,\n    prompt_registry=prompt_registry,\n    enable_logging=True  # Enable Rich logging\n)\n\n# Run the agent - you'll see colorized output showing:\n# - User messages (cyan)\n# - LLM calls (dim)\n# - Tool calls (magenta)\n# - Tool responses (green)\n# - Assistant responses (bold green)\nresponse = agent.run(\"Summarize the benefits of good logging in one sentence.\")\nprint(response)\n</code></pre>"},{"location":"how-to/#custom-logger","title":"Custom Logger","text":"<p>You can also provide a custom logger instance:</p> <pre><code>import os\nimport logging\nfrom iris_agent import Agent, SyncLLMClient, LLMConfig, LLMProvider, PromptRegistry\n\n# Create a custom logger\ncustom_logger = logging.getLogger(\"my_agent\")\ncustom_logger.setLevel(logging.INFO)\nhandler = logging.StreamHandler()\nhandler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))\ncustom_logger.addHandler(handler)\n\n# Create client and agent\nclient = SyncLLMClient(LLMConfig(\n    provider=LLMProvider.OPENAI,\n    model=\"gpt-4o-mini\",\n    api_key=os.getenv(\"OPENAI_API_KEY\")\n))\nprompt_registry = PromptRegistry()\nprompt_registry.add_prompt(\"assistant\", \"You are a helpful assistant.\")\n\n# Use custom logger\nagent = Agent(\n    llm_client=client,\n    prompt_registry=prompt_registry,\n    enable_logging=True,\n    logger=custom_logger\n)\n\nresponse = agent.run(\"Hello!\")\nprint(response)\n</code></pre>"},{"location":"installation/","title":"Installation Guide","text":""},{"location":"installation/#system-requirements","title":"System Requirements","text":"<ul> <li>OS: macOS, Linux, or Windows</li> <li>Python: Version 3.10 or newer</li> </ul>"},{"location":"installation/#installing-via-pip","title":"Installing via pip","text":"<p>The easiest way to install Iris Agent is via <code>pip</code> from PyPI:</p> <pre><code>pip install iris-agent\n</code></pre>"},{"location":"installation/#installing-from-source","title":"Installing from Source","text":"<p>If you want the latest development version or want to contribute:</p> <ol> <li> <p>Clone the repository:     <pre><code>git clone https://github.com/mrgehlot/iris-agent.git\ncd iris-agent\n</code></pre></p> </li> <li> <p>Create a virtual environment (recommended):     <pre><code>python -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n</code></pre></p> </li> <li> <p>Install in editable mode:     <pre><code>pip install -e .\n</code></pre></p> </li> </ol>"},{"location":"installation/#installing-development-dependencies","title":"Installing Development Dependencies","text":"<p>If you plan to run tests or build documentation:</p> <pre><code>pip install -e \".[dev]\"\n</code></pre> <p>This installs tools like <code>pytest</code>, <code>black</code>, <code>isort</code>, <code>mkdocs</code>, <code>mkdocs-material</code>.</p>"},{"location":"installation/#verifying-installation","title":"Verifying Installation","text":"<p>To check if the installation was successful, run a quick import in Python:</p> <pre><code>python -c \"import iris_agent; print(f'Iris Agent {iris_agent.__file__} installed successfully')\"\n</code></pre>"},{"location":"installation/#troubleshooting-installation","title":"Troubleshooting Installation","text":"<p>\"Module not found\" error: Ensure your virtual environment is activated.</p> <p>Asyncio errors: Iris Agent relies heavily on <code>asyncio</code>. Ensure you are not trying to nest <code>asyncio.run()</code> calls if you are using the synchronous <code>Agent</code> inside an already running event loop (e.g., in a Jupyter Notebook). For those cases, use <code>AsyncAgent</code>.</p>"},{"location":"modules/","title":"Modules Reference","text":"<p>The <code>iris_agent</code> package is structured as follows:</p>"},{"location":"modules/#srciris_agent","title":"<code>src/iris_agent/</code>","text":"<p>The core package source.</p> Module Description <code>agent.py</code> Contains the <code>Agent</code> class (Synchronous agent). <code>async_agent.py</code> Contains the <code>AsyncAgent</code> class (Core logic). <code>llm.py</code> Contains <code>BaseLLMClient</code>, <code>SyncLLMClient</code>, <code>AsyncLLMClient</code>, and provider logic. <code>tools.py</code> Contains <code>ToolRegistry</code>, <code>@tool</code> decorator, and schema inference logic. <code>prompts.py</code> Contains <code>PromptRegistry</code> for managing system prompts. <code>messages.py</code> Helpers for creating messages (text, images). <code>types.py</code> Type definitions and constants (e.g., <code>Role</code>)."},{"location":"modules/#examples","title":"<code>examples/</code>","text":"<p>Contains reference implementations.</p> <ul> <li><code>01_basic/</code>: Simple chat and streaming examples.</li> <li><code>02_prompts/</code>: Using the prompt registry.</li> <li><code>03_tools/</code>: Tool creation and usage.</li> <li><code>04_memory/</code>: Memory inspection.</li> <li><code>05_multi_agent/</code>: Patterns for multiple agents.</li> <li><code>09_gemini/</code>: Specific examples for Google Gemini.</li> </ul>"},{"location":"modules/#tests","title":"<code>tests/</code>","text":"<p>Unit and integration tests using <code>pytest</code>.</p>"},{"location":"troubleshooting/","title":"Troubleshooting","text":""},{"location":"troubleshooting/#common-issues","title":"Common Issues","text":""},{"location":"troubleshooting/#1-runtimeerror-agentrun-called-inside-an-event-loop","title":"1. \"RuntimeError: Agent.run called inside an event loop\"","text":"<p>Cause: You are using the synchronous <code>Agent</code> class inside an async environment (like a Jupyter Notebook or a FastAPI route).</p> <p>Solution: Use <code>AsyncAgent</code> instead.</p> <pre><code># Bad\nagent = Agent(...)\nagent.run(\"hi\")\n\n# Good\nagent = AsyncAgent(...)\nawait agent.run(\"hi\")\n</code></pre>"},{"location":"troubleshooting/#2-tools-are-not-being-called","title":"2. Tools are not being called","text":"<p>Cause: - The tool might not be registered with the <code>ToolRegistry</code>. - The <code>ToolRegistry</code> was not passed to the <code>Agent</code> constructor. - The tool's docstring description is too vague for the LLM to understand when to use it.</p> <p>Solution: - Check registration: <code>print(registry.list_tools())</code>. - Improve docstrings: Explain what the tool does and when to use it.</p>"},{"location":"troubleshooting/#3-api-connection-errors","title":"3. API Connection Errors","text":"<p>Cause: Missing API key or incorrect Base URL.</p> <p>Solution: - Ensure <code>OPENAI_API_KEY</code> (or equivalent) is set. - If using a local model (e.g., Ollama), ensure <code>base_url</code> is correct (e.g., <code>http://localhost:11434/v1</code>).</p>"},{"location":"troubleshooting/#4-import-errors","title":"4. Import Errors","text":"<p>Cause: The package is not installed in the current environment.</p> <p>Solution: Run <code>pip install .</code> or <code>pip install -e .</code> in the project root.</p>"}]}